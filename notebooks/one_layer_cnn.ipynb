{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27b376de",
   "metadata": {},
   "source": [
    "# MRI vs Breast Histopathology - Part 2: Model & Training (One-Layer CNN)\n",
    "\n",
    "This notebook implements the model and training pipeline for a binary classifier distinguishing MRI vs BreastHisto images, using a simple one-layer CNN.\n",
    "\n",
    "**Workflow:**\n",
    "- Part 1 (04_mri_vs_breasthisto_part1_data_preparation.ipynb): Data organization, grayscale conversion, and train/val/test splitting\n",
    "- Part 2 (this notebook): Model training with preprocessed grayscale images converted to 3-channel format\n",
    "\n",
    "**Data Pipeline:**\n",
    "1. Load preprocessed grayscale PNG images from data/processed\n",
    "2. Convert grayscale to 3-channel (repeat across RGB channels)\n",
    "3. Apply ImageNet normalization for model compatibility\n",
    "4. Train a one-layer CNN with BCE loss for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718da98a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard libs\n",
    "from __future__ import annotations\n",
    "import os, sys, json, random\n",
    "from dataclasses import dataclass, asdict\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from typing import Dict\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Paths\n",
    "PROJECT_ROOT = Path.cwd().parent\n",
    "DATA_PROCESSED = PROJECT_ROOT / 'data' / 'processed'\n",
    "MODELS_DIR = PROJECT_ROOT / 'models'\n",
    "OUTPUTS_DIR = PROJECT_ROOT / 'outputs'\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "OUTPUTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Device:', DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fba471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load preparation metadata\n",
    "meta_path = DATA_PROCESSED / 'preparation_metadata.json'\n",
    "if meta_path.exists():\n",
    "    with open(meta_path, 'r') as f:\n",
    "        PREP_META = json.load(f)\n",
    "    print('Loaded metadata:', PREP_META.get('processed_data_path'))\n",
    "else:\n",
    "    PREP_META = {\n",
    "        'processed_data_path': str(DATA_PROCESSED),\n",
    "        'ready_for_training': False\n",
    "    }\n",
    "    print('Warning: preparation_metadata.json not found. Using defaults.')\n",
    "\n",
    "@dataclass\n",
    "class TrainConfig:\n",
    "    # Data configuration\n",
    "    image_size: int = 224\n",
    "    batch_size: int = 32\n",
    "    num_workers: int = 0  # Windows-safe\n",
    "    \n",
    "    # Training configuration\n",
    "    epochs: int = 15\n",
    "    learning_rate: float = 1e-3\n",
    "    weight_decay: float = 1e-4\n",
    "    \n",
    "    # Scheduler configuration\n",
    "    scheduler: str = 'cosine'  # 'cosine' | 'plateau'\n",
    "    \n",
    "    # Early stopping and model saving\n",
    "    save_best_metric: str = 'val_f1'  # 'val_accuracy' | 'val_f1' | 'val_auc'\n",
    "    patience: int = 5\n",
    "\n",
    "CONFIG = TrainConfig()\n",
    "CLASS_NAMES = ['MRI', 'BreastHisto']\n",
    "CLASS_TO_IDX = {c:i for i,c in enumerate(CLASS_NAMES)}\n",
    "IDX_TO_CLASS = {i:c for c,i in CLASS_TO_IDX.items()}\n",
    "\n",
    "print('Training Configuration:')\n",
    "print(f'  Image Size: {CONFIG.image_size}x{CONFIG.image_size}')\n",
    "print(f'  Batch Size: {CONFIG.batch_size}')\n",
    "print(f'  Max Epochs: {CONFIG.epochs}')\n",
    "print(f'  Classes: {CLASS_NAMES} -> {CLASS_TO_IDX}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0af65a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization values for 3-channel compatibility\n",
    "RGB_MEAN = [0.485, 0.456, 0.406]\n",
    "RGB_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def load_image(path: str) -> np.ndarray:\n",
    "    \"\"\"Load preprocessed grayscale PNG image\"\"\"\n",
    "    img = cv2.imread(path, cv2.IMREAD_GRAYSCALE)\n",
    "    if img is None:\n",
    "        img = np.array(Image.open(path).convert('L'))\n",
    "    return img\n",
    "\n",
    "def to_tensor(img: np.ndarray, size: int) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Convert preprocessed grayscale image to 3-channel tensor with proper normalization\n",
    "    Steps:\n",
    "    1. Resize to target size (if needed)\n",
    "    2. Normalize to [0, 1]\n",
    "    3. Convert grayscale to 3-channel by repeating\n",
    "    4. Apply ImageNet normalization per channel\n",
    "    \"\"\"\n",
    "    # Resize and normalize to [0, 1]\n",
    "    img = cv2.resize(img, (size, size), interpolation=cv2.INTER_LINEAR).astype(np.float32) / 255.0\n",
    "    \n",
    "    # Convert to 3-channel by repeating grayscale across RGB channels\n",
    "    img_3ch = np.stack([img, img, img], axis=0)  # Shape: (3, H, W)\n",
    "    \n",
    "    # Apply per-channel normalization (ImageNet stats)\n",
    "    for i in range(3):\n",
    "        img_3ch[i] = (img_3ch[i] - RGB_MEAN[i]) / RGB_STD[i]\n",
    "    \n",
    "    return torch.from_numpy(img_3ch)\n",
    "\n",
    "class BinaryMedicalDataset(Dataset):\n",
    "    \"\"\"Dataset for loading preprocessed binary medical images\"\"\"\n",
    "    def __init__(self, root: Path, split: str, size: int = 224):\n",
    "        self.root = Path(root)\n",
    "        self.split = split\n",
    "        self.size = size\n",
    "        self.samples = []\n",
    "        \n",
    "        for cls in CLASS_NAMES:\n",
    "            cdir = self.root / split / cls\n",
    "            if not cdir.exists():\n",
    "                continue\n",
    "            for ext in ('*.png','*.jpg','*.jpeg','*.bmp','*.tif','*.tiff'):\n",
    "                for p in cdir.glob(ext):\n",
    "                    self.samples.append((str(p), CLASS_TO_IDX[cls]))\n",
    "        \n",
    "        random.shuffle(self.samples)\n",
    "        self.counts = {c:0 for c in CLASS_NAMES}\n",
    "        for _,lbl in self.samples:\n",
    "            self.counts[IDX_TO_CLASS[lbl]] += 1\n",
    "        print(f\"Loaded {split}: {len(self.samples)} | {self.counts}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        path, label = self.samples[idx]\n",
    "        try:\n",
    "            img = load_image(path)\n",
    "        except Exception:\n",
    "            img = np.zeros((CONFIG.image_size, CONFIG.image_size), dtype=np.uint8)\n",
    "        x = to_tensor(img, CONFIG.image_size)\n",
    "        y = torch.tensor(float(label), dtype=torch.float32)\n",
    "        return x, y\n",
    "\n",
    "def create_loaders(data_root: Path, cfg: TrainConfig):\n",
    "    ds_tr = BinaryMedicalDataset(data_root, 'train', cfg.image_size)\n",
    "    ds_va = BinaryMedicalDataset(data_root, 'val', cfg.image_size)\n",
    "    ds_te = BinaryMedicalDataset(data_root, 'test', cfg.image_size)\n",
    "    dl_tr = DataLoader(ds_tr, batch_size=cfg.batch_size, shuffle=True, num_workers=cfg.num_workers, pin_memory=True, drop_last=True)\n",
    "    dl_va = DataLoader(ds_va, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    dl_te = DataLoader(ds_te, batch_size=cfg.batch_size, shuffle=False, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    return {'train': dl_tr, 'val': dl_va, 'test': dl_te}\n",
    "\n",
    "class OneLayerCNN(nn.Module):\n",
    "    \"\"\"Simple CNN with 1 convolutional layer for binary classification\"\"\"\n",
    "    def __init__(self, in_channels: int = 3, num_classes: int = 1):\n",
    "        super(OneLayerCNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction: 1 convolutional block\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 16, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(16), # Adding BatchNorm is a good practice\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)  # 224x224 -> 112x112\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.5), # Adding Dropout is a good practice\n",
    "            # The number of input features to the linear layer is 16 * 112 * 112\n",
    "            nn.Linear(16 * 112 * 112, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._initialize_weights()\n",
    "    \n",
    "    def _initialize_weights(self):\n",
    "        \"\"\"Initialize model weights\"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                nn.init.normal_(m.weight, 0, 0.01)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x.squeeze(-1)  # Remove last dimension for binary classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a79978",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(y_true: torch.Tensor, y_prob: torch.Tensor):\n",
    "    y_pred = (y_prob >= 0.5).float()\n",
    "    yt = y_true.cpu().numpy(); yp = y_pred.cpu().numpy(); ypb = y_prob.cpu().numpy()\n",
    "    acc = accuracy_score(yt, yp)\n",
    "    pr, rc, f1, _ = precision_recall_fscore_support(yt, yp, average='binary', zero_division=0)\n",
    "    try:\n",
    "        auc = roc_auc_score(yt, ypb)\n",
    "    except Exception:\n",
    "        auc = 0.0\n",
    "    return {'accuracy': acc, 'precision': pr, 'recall': rc, 'f1': f1, 'auc': auc}\n",
    "\n",
    "def train_one_epoch(model, loader, criterion, optimizer):\n",
    "    \"\"\"Train model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    y_true_list = []\n",
    "    y_prob_list = []\n",
    "    \n",
    "    for x, y in loader:\n",
    "        x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        y_true_list.append(y.detach().cpu())\n",
    "        y_prob_list.append(torch.sigmoid(logits).detach().cpu())\n",
    "    \n",
    "    # Compute epoch metrics\n",
    "    y_true_tensor = torch.cat(y_true_list)\n",
    "    y_prob_tensor = torch.cat(y_prob_list)\n",
    "    metrics = compute_metrics(y_true_tensor, y_prob_tensor)\n",
    "    metrics['loss'] = total_loss / len(loader)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def evaluate(model, loader, criterion):\n",
    "    \"\"\"Evaluate model\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0.0\n",
    "    y_true_list = []\n",
    "    y_prob_list = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            \n",
    "            # Forward pass\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            \n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            y_true_list.append(y.detach().cpu())\n",
    "            y_prob_list.append(torch.sigmoid(logits).detach().cpu())\n",
    "    \n",
    "    # Compute metrics\n",
    "    y_true_tensor = torch.cat(y_true_list)\n",
    "    y_prob_tensor = torch.cat(y_prob_list)\n",
    "    metrics = compute_metrics(y_true_tensor, y_prob_tensor)\n",
    "    metrics['loss'] = total_loss / len(loader)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def train_pipeline():\n",
    "    \"\"\"Main training pipeline\"\"\"\n",
    "    print(\"Starting Training Pipeline\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Load data\n",
    "    print(\"Loading datasets...\")\n",
    "    data_root = Path(PREP_META.get('processed_data_path', DATA_PROCESSED))\n",
    "    loaders = create_loaders(data_root, CONFIG)\n",
    "    \n",
    "    # Initialize model\n",
    "    print(\"Initializing model...\")\n",
    "    model = OneLayerCNN(in_channels=3, num_classes=1).to(DEVICE)\n",
    "    \n",
    "    # Initialize training components\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=CONFIG.learning_rate, weight_decay=CONFIG.weight_decay)\n",
    "    \n",
    "    if CONFIG.scheduler == 'cosine':\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer, T_max=CONFIG.epochs, eta_min=CONFIG.learning_rate*0.01\n",
    "        )\n",
    "    else:\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer, mode='min', factor=0.5, patience=3\n",
    "        )\n",
    "    \n",
    "    # Training setup\n",
    "    history = {\n",
    "        'train_loss': [], 'train_accuracy': [], 'train_f1': [],\n",
    "        'val_loss': [], 'val_accuracy': [], 'val_f1': [], 'val_auc': []\n",
    "    }\n",
    "    \n",
    "    best_metric = 0.0\n",
    "    patience_counter = 0\n",
    "    best_path = MODELS_DIR / 'best_onelayercnn_binary.pth'\n",
    "    \n",
    "    print(\"Starting training...\")\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(CONFIG.epochs):\n",
    "        print(f\"Epoch {epoch+1}/{CONFIG.epochs}\")\n",
    "        \n",
    "        # Training phase\n",
    "        train_metrics = train_one_epoch(model, loaders['train'], criterion, optimizer)\n",
    "        \n",
    "        # Validation phase\n",
    "        val_metrics = evaluate(model, loaders['val'], criterion)\n",
    "        \n",
    "        # Learning rate update\n",
    "        if isinstance(scheduler, optim.lr_scheduler.ReduceLROnPlateau):\n",
    "            scheduler.step(val_metrics['loss'])\n",
    "        else:\n",
    "            scheduler.step()\n",
    "        \n",
    "        # Update history\n",
    "        history['train_loss'].append(train_metrics['loss'])\n",
    "        history['train_accuracy'].append(train_metrics['accuracy'])\n",
    "        history['train_f1'].append(train_metrics['f1'])\n",
    "        \n",
    "        history['val_loss'].append(val_metrics['loss'])\n",
    "        history['val_accuracy'].append(val_metrics['accuracy'])\n",
    "        history['val_f1'].append(val_metrics['f1'])\n",
    "        history['val_auc'].append(val_metrics['auc'])\n",
    "        \n",
    "        # Select metric for best model saving\n",
    "        if CONFIG.save_best_metric == 'val_f1':\n",
    "            current_metric = val_metrics['f1']\n",
    "        elif CONFIG.save_best_metric == 'val_accuracy':\n",
    "            current_metric = val_metrics['accuracy']\n",
    "        else:\n",
    "            current_metric = val_metrics['auc']\n",
    "        \n",
    "        # Best model saving and early stopping\n",
    "        if current_metric > best_metric + 1e-4:\n",
    "            best_metric = current_metric\n",
    "            patience_counter = 0\n",
    "            \n",
    "            # Save best model\n",
    "            checkpoint = {\n",
    "                'epoch': epoch + 1,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'config': asdict(CONFIG),\n",
    "                'history': history,\n",
    "                'best_metric': best_metric,\n",
    "                'best_metric_name': CONFIG.save_best_metric\n",
    "            }\n",
    "            torch.save(checkpoint, best_path)\n",
    "            print(f\"New best {CONFIG.save_best_metric}: {current_metric:.4f} - Model saved!\")\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        # Epoch summary\n",
    "        print(f\"Train -> Loss: {train_metrics['loss']:.4f}, Acc: {train_metrics['accuracy']:.3f}, F1: {train_metrics['f1']:.3f}\")\n",
    "        print(f\"Val   -> Loss: {val_metrics['loss']:.4f}, Acc: {val_metrics['accuracy']:.3f}, F1: {val_metrics['f1']:.3f}, AUC: {val_metrics['auc']:.3f}\")\n",
    "        print(f\"Best {CONFIG.save_best_metric}: {best_metric:.4f}, Patience: {patience_counter}/{CONFIG.patience}\")\n",
    "        \n",
    "        # Early stopping check\n",
    "        if patience_counter >= CONFIG.patience:\n",
    "            print(f\"Early stopping triggered after {patience_counter} epochs without improvement\")\n",
    "            break\n",
    "    \n",
    "    # Training completion\n",
    "    print(f\"\\nTraining Complete!\")\n",
    "    print(f\"Best {CONFIG.save_best_metric}: {best_metric:.4f}\")\n",
    "    print(f\"Best model saved to: {best_path}\")\n",
    "    \n",
    "    # Save training history\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    history_path = OUTPUTS_DIR / f\"training_history_onelayer_{timestamp}.json\"\n",
    "    \n",
    "    # Convert numpy types to Python types for JSON serialization\n",
    "    history_serializable = {}\n",
    "    for key, values in history.items():\n",
    "        history_serializable[key] = [float(v) if hasattr(v, 'item') else v for v in values]\n",
    "    \n",
    "    training_summary = {\n",
    "        'config': asdict(CONFIG),\n",
    "        'training_history': history_serializable,\n",
    "        'best_metric': float(best_metric),\n",
    "        'best_metric_name': CONFIG.save_best_metric,\n",
    "        'total_epochs': epoch + 1,\n",
    "        'timestamp': timestamp\n",
    "    }\n",
    "    \n",
    "    with open(history_path, 'w') as f:\n",
    "        json.dump(training_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"Training history saved to: {history_path}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca22fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training\n",
    "if __name__ == \"__main__\":\n",
    "    history = train_pipeline()\n",
    "    print('\\nTraining complete. Best model saved in models/.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_tumor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

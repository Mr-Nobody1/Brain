{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b229699",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4a5e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from collections import defaultdict, Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image, ImageFile\n",
    "import cv2\n",
    "\n",
    "# PyTorch ecosystem\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "\n",
    "# Torchvision\n",
    "import torchvision\n",
    "from torchvision import transforms, models\n",
    "from torchvision.models import EfficientNet_B0_Weights, ResNet18_Weights\n",
    "\n",
    "# Albumentations for advanced augmentations\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "# Metrics and evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support,\n",
    "    confusion_matrix, classification_report, roc_auc_score, roc_curve\n",
    ")\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.cuda.manual_seed_all(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"CUDA Version: {torch.version.cuda}\")\n",
    "    print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "print(\"✓ All libraries imported and environment configured\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8657720",
   "metadata": {},
   "source": [
    "## 2. Configuration and Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e525baaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / 'data'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "models_dir = project_root / 'models'\n",
    "outputs_dir = project_root / 'outputs'\n",
    "\n",
    "# Create output directories\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "outputs_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(f\"Project root: {project_root}\")\n",
    "print(f\"Data directory: {processed_data_dir}\")\n",
    "print(f\"Models directory: {models_dir}\")\n",
    "print(f\"Outputs directory: {outputs_dir}\")\n",
    "\n",
    "# Training configuration\n",
    "CONFIG = {\n",
    "    # Data settings\n",
    "    'image_size': 224,\n",
    "    'batch_size': 32,  # Start conservative, can increase to 64\n",
    "    'num_workers': 0,  # Set to 0 to avoid deadlocks on Windows\n",
    "    'pin_memory': True,\n",
    "    \n",
    "    # Model settings\n",
    "    'model_name': 'efficientnet_b0',  # efficientnet_b0, resnet18, custom_cnn\n",
    "    'pretrained': True,\n",
    "    'num_classes': 1,  # Binary classification with single output + sigmoid\n",
    "    \n",
    "    # Training settings\n",
    "    'epochs': 50,\n",
    "    'learning_rate': 3e-4,\n",
    "    'weight_decay': 1e-4,\n",
    "    'patience': 7,  # Early stopping patience\n",
    "    \n",
    "    # Transfer learning settings\n",
    "    'freeze_epochs': 5,  # Train only classifier head\n",
    "    'finetune_lr_factor': 0.1,  # LR reduction for fine-tuning\n",
    "    \n",
    "    # Scheduler settings\n",
    "    'scheduler_type': 'cosine',  # cosine, plateau\n",
    "    'plateau_patience': 3,\n",
    "    'plateau_factor': 0.5,\n",
    "    \n",
    "    # Evaluation\n",
    "    'save_best_metric': 'val_f1',  # val_accuracy, val_f1, val_auc\n",
    "    'print_freq': 50,  # Print every N batches\n",
    "    \n",
    "    # Reproducibility\n",
    "    'seed': SEED\n",
    "}\n",
    "\n",
    "# Class mapping\n",
    "CLASS_NAMES = ['MRI', 'BreastHisto']\n",
    "CLASS_TO_IDX = {name: idx for idx, name in enumerate(CLASS_NAMES)}\n",
    "IDX_TO_CLASS = {idx: name for name, idx in CLASS_TO_IDX.items()}\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "for key, value in CONFIG.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "print(f\"\\nClass mapping: {CLASS_TO_IDX}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee6b83e",
   "metadata": {},
   "source": [
    "## 3. Dataset Statistics and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81713107",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_dataset_structure():\n",
    "    \"\"\"Analyze the processed dataset structure and statistics\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATASET ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    splits = ['train', 'val', 'test']\n",
    "    classes = ['MRI', 'BreastHisto']\n",
    "    \n",
    "    dataset_stats = {}\n",
    "    total_images = 0\n",
    "    \n",
    "    for split in splits:\n",
    "        split_dir = processed_data_dir / split\n",
    "        split_stats = {}\n",
    "        split_total = 0\n",
    "        \n",
    "        print(f\"\\n{split.upper()} SET:\")\n",
    "        \n",
    "        for class_name in classes:\n",
    "            class_dir = split_dir / class_name\n",
    "            if class_dir.exists():\n",
    "                # Count image files\n",
    "                image_files = []\n",
    "                for ext in ['*.jpg', '*.jpeg', '*.png', '*.tiff', '*.tif', '*.bmp']:\n",
    "                    image_files.extend(list(class_dir.glob(ext)))\n",
    "                \n",
    "                count = len(image_files)\n",
    "                split_stats[class_name] = count\n",
    "                split_total += count\n",
    "                \n",
    "                print(f\"  {class_name}: {count:,} images\")\n",
    "            else:\n",
    "                split_stats[class_name] = 0\n",
    "                print(f\"  {class_name}: Directory not found\")\n",
    "        \n",
    "        split_stats['total'] = split_total\n",
    "        dataset_stats[split] = split_stats\n",
    "        total_images += split_total\n",
    "        \n",
    "        print(f\"  Total {split}: {split_total:,} images\")\n",
    "        \n",
    "        # Calculate class balance\n",
    "        if split_total > 0:\n",
    "            for class_name in classes:\n",
    "                percentage = (split_stats[class_name] / split_total) * 100\n",
    "                print(f\"    {class_name}: {percentage:.1f}%\")\n",
    "    \n",
    "    print(f\"\\nOVERALL STATISTICS:\")\n",
    "    print(f\"  Total images: {total_images:,}\")\n",
    "    \n",
    "    # Calculate split percentages\n",
    "    for split in splits:\n",
    "        if total_images > 0:\n",
    "            percentage = (dataset_stats[split]['total'] / total_images) * 100\n",
    "            print(f\"  {split}: {percentage:.1f}%\")\n",
    "    \n",
    "    return dataset_stats\n",
    "\n",
    "dataset_stats = analyze_dataset_structure()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea7bf22",
   "metadata": {},
   "source": [
    "## 4. Image Preprocessing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8734416",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ImageNet normalization values (for pretrained models)\n",
    "IMAGENET_MEAN = [0.485, 0.456, 0.406]\n",
    "IMAGENET_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "def get_transforms(split='train', image_size=224):\n",
    "    \"\"\"\n",
    "    Get preprocessing transforms for different splits\n",
    "    \n",
    "    Train: Aggressive augmentations for generalization\n",
    "    Val/Test: Only essential preprocessing (resize + normalize)\n",
    "    \"\"\"\n",
    "    \n",
    "    if split == 'train':\n",
    "        # Training transforms with augmentations\n",
    "        transform = A.Compose([\n",
    "            # Resize to target size\n",
    "            A.Resize(image_size, image_size, interpolation=cv2.INTER_LINEAR),\n",
    "            \n",
    "            # Geometric augmentations\n",
    "            A.HorizontalFlip(p=0.5),\n",
    "            A.VerticalFlip(p=0.3),  # Less common but useful for medical images\n",
    "            A.Rotate(limit=15, p=0.7, border_mode=cv2.BORDER_CONSTANT, value=0),\n",
    "            \n",
    "            # Crop and zoom augmentations\n",
    "            A.RandomResizedCrop(\n",
    "                size=(image_size, image_size),\n",
    "                scale=(0.8, 1.0),  # Zoom range\n",
    "                ratio=(0.9, 1.1),  # Aspect ratio range\n",
    "                p=0.7\n",
    "            ),\n",
    "            \n",
    "            # Color augmentations (subtle for medical images)\n",
    "            A.RandomBrightnessContrast(\n",
    "                brightness_limit=0.1,  # ±10% brightness\n",
    "                contrast_limit=0.1,    # ±10% contrast\n",
    "                p=0.5\n",
    "            ),\n",
    "            \n",
    "            # Optional noise and blur (very gentle)\n",
    "            A.OneOf([\n",
    "                A.GaussNoise(var_limit=(5.0, 15.0), p=1.0),\n",
    "                A.GaussianBlur(blur_limit=(1, 3), p=1.0),\n",
    "                A.MotionBlur(blur_limit=(3, 5), p=1.0),\n",
    "            ], p=0.3),\n",
    "            \n",
    "            # Normalization and tensor conversion\n",
    "            A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "        \n",
    "    else:\n",
    "        # Validation/Test transforms (no augmentation)\n",
    "        transform = A.Compose([\n",
    "            A.Resize(image_size, image_size, interpolation=cv2.INTER_LINEAR),\n",
    "            A.Normalize(mean=IMAGENET_MEAN, std=IMAGENET_STD),\n",
    "            ToTensorV2()\n",
    "        ])\n",
    "    \n",
    "    return transform\n",
    "\n",
    "def convert_to_3_channel(image):\n",
    "    \"\"\"Convert grayscale or other formats to 3-channel RGB\"\"\"\n",
    "    if len(image.shape) == 2:  # Grayscale\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "    elif len(image.shape) == 3:\n",
    "        if image.shape[2] == 1:  # Single channel\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
    "        elif image.shape[2] == 4:  # RGBA\n",
    "            image = cv2.cvtColor(image, cv2.COLOR_RGBA2RGB)\n",
    "        elif image.shape[2] == 3:  # Already RGB\n",
    "            pass\n",
    "        else:\n",
    "            # Handle other channel counts by duplicating first channel\n",
    "            image = np.stack([image[:, :, 0]] * 3, axis=2)\n",
    "    \n",
    "    return image\n",
    "\n",
    "# Test transforms\n",
    "print(\"Testing transform pipeline...\")\n",
    "\n",
    "# Create sample transforms\n",
    "train_transform = get_transforms('train', CONFIG['image_size'])\n",
    "val_transform = get_transforms('val', CONFIG['image_size'])\n",
    "\n",
    "print(f\"✓ Transform pipeline created\")\n",
    "print(f\"  Train augmentations: {len(train_transform.transforms)} steps\")\n",
    "print(f\"  Val/Test preprocessing: {len(val_transform.transforms)} steps\")\n",
    "print(f\"  Target size: {CONFIG['image_size']}x{CONFIG['image_size']}\")\n",
    "print(f\"  Normalization: ImageNet mean/std\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20923744",
   "metadata": {},
   "source": [
    "## 5. Custom Dataset Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd2f438",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BinaryMedicalDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom dataset for binary classification between MRI and BreastHisto images\n",
    "    \n",
    "    Features:\n",
    "    - Automatic 3-channel conversion\n",
    "    - Robust image loading with error handling\n",
    "    - Class balancing information\n",
    "    - Memory-efficient loading\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir, split='train', transform=None, class_to_idx=None):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.split = split\n",
    "        self.transform = transform\n",
    "        self.class_to_idx = class_to_idx or CLASS_TO_IDX\n",
    "        \n",
    "        # Collect all image paths and labels\n",
    "        self.samples = []\n",
    "        self.class_counts = Counter()\n",
    "        \n",
    "        self._load_samples()\n",
    "        \n",
    "        print(f\"Loaded {split} dataset: {len(self.samples):,} images\")\n",
    "        for class_name, count in self.class_counts.items():\n",
    "            print(f\"  {class_name}: {count:,} images ({count/len(self.samples)*100:.1f}%)\")\n",
    "    \n",
    "    def _load_samples(self):\n",
    "        \"\"\"Load all image paths and corresponding labels\"\"\"\n",
    "        split_dir = self.data_dir / self.split\n",
    "        \n",
    "        for class_name in self.class_to_idx.keys():\n",
    "            class_dir = split_dir / class_name\n",
    "            \n",
    "            if not class_dir.exists():\n",
    "                print(f\"⚠️  Warning: {class_dir} does not exist\")\n",
    "                continue\n",
    "            \n",
    "            # Supported image extensions\n",
    "            image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.tiff', '*.tif', '*.bmp']\n",
    "            \n",
    "            for ext in image_extensions:\n",
    "                for img_path in class_dir.glob(ext):\n",
    "                    label = self.class_to_idx[class_name]\n",
    "                    self.samples.append((str(img_path), label))\n",
    "                    self.class_counts[class_name] += 1\n",
    "        \n",
    "        # Shuffle samples for better training\n",
    "        random.shuffle(self.samples)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.samples[idx]\n",
    "        \n",
    "        try:\n",
    "            # Load image\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                # Fallback to PIL\n",
    "                image = np.array(Image.open(img_path))\n",
    "            \n",
    "            # Convert BGR to RGB if loaded with cv2\n",
    "            if len(image.shape) == 3 and image.shape[2] == 3:\n",
    "                image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "            \n",
    "            # Ensure 3 channels\n",
    "            image = convert_to_3_channel(image)\n",
    "            \n",
    "            # Apply transforms\n",
    "            if self.transform:\n",
    "                transformed = self.transform(image=image)\n",
    "                image = transformed['image']\n",
    "            else:\n",
    "                # Convert to tensor if no transforms\n",
    "                image = torch.from_numpy(image.transpose(2, 0, 1)).float() / 255.0\n",
    "            \n",
    "            return image, torch.tensor(label, dtype=torch.float32)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {img_path}: {e}\")\n",
    "            # Return a black image as fallback\n",
    "            if self.transform:\n",
    "                black_image = np.zeros((CONFIG['image_size'], CONFIG['image_size'], 3), dtype=np.uint8)\n",
    "                transformed = self.transform(image=black_image)\n",
    "                image = transformed['image']\n",
    "            else:\n",
    "                image = torch.zeros(3, CONFIG['image_size'], CONFIG['image_size'])\n",
    "            \n",
    "            return image, torch.tensor(label, dtype=torch.float32)\n",
    "    \n",
    "    def get_class_weights(self):\n",
    "        \"\"\"Calculate class weights for balanced training\"\"\"\n",
    "        total_samples = len(self.samples)\n",
    "        num_classes = len(self.class_to_idx)\n",
    "        \n",
    "        weights = []\n",
    "        for class_name in self.class_to_idx.keys():\n",
    "            class_count = self.class_counts[class_name]\n",
    "            weight = total_samples / (num_classes * class_count) if class_count > 0 else 0\n",
    "            weights.append(weight)\n",
    "        \n",
    "        return torch.tensor(weights, dtype=torch.float32)\n",
    "\n",
    "# Test dataset loading\n",
    "print(\"Testing dataset loading...\")\n",
    "\n",
    "try:\n",
    "    # Load a small sample to test\n",
    "    train_transform = get_transforms('train', CONFIG['image_size'])\n",
    "    \n",
    "    train_dataset = BinaryMedicalDataset(\n",
    "        processed_data_dir, \n",
    "        split='train', \n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    # Test loading a sample\n",
    "    if len(train_dataset) > 0:\n",
    "        sample_image, sample_label = train_dataset[0]\n",
    "        print(f\"✓ Dataset loading successful\")\n",
    "        print(f\"  Sample image shape: {sample_image.shape}\")\n",
    "        print(f\"  Sample label: {sample_label.item()} ({IDX_TO_CLASS[int(sample_label.item())]})\")\n",
    "        print(f\"  Image tensor range: [{sample_image.min():.3f}, {sample_image.max():.3f}]\")\n",
    "    else:\n",
    "        print(\"⚠️  Dataset is empty\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error testing dataset: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c9d811",
   "metadata": {},
   "source": [
    "## 6. Data Loaders Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c184107c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loaders(data_dir, config):\n",
    "    \"\"\"Create train, validation, and test data loaders\"\"\"\n",
    "    \n",
    "    # Create transforms\n",
    "    train_transform = get_transforms('train', config['image_size'])\n",
    "    val_transform = get_transforms('val', config['image_size'])\n",
    "    \n",
    "    # Create datasets\n",
    "    datasets = {}\n",
    "    datasets['train'] = BinaryMedicalDataset(\n",
    "        data_dir, 'train', transform=train_transform\n",
    "    )\n",
    "    datasets['val'] = BinaryMedicalDataset(\n",
    "        data_dir, 'val', transform=val_transform\n",
    "    )\n",
    "    datasets['test'] = BinaryMedicalDataset(\n",
    "        data_dir, 'test', transform=val_transform\n",
    "    )\n",
    "    \n",
    "    # Create data loaders\n",
    "    dataloaders = {}\n",
    "    \n",
    "    # Training loader with shuffling\n",
    "    dataloaders['train'] = DataLoader(\n",
    "        datasets['train'],\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=True,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=config['pin_memory'],\n",
    "        drop_last=True  # Drop last incomplete batch for training\n",
    "    )\n",
    "    \n",
    "    # Validation loader without shuffling\n",
    "    dataloaders['val'] = DataLoader(\n",
    "        datasets['val'],\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=config['pin_memory'],\n",
    "        drop_last=False  # Keep all data for evaluation\n",
    "    )\n",
    "    \n",
    "    # Test loader without shuffling\n",
    "    dataloaders['test'] = DataLoader(\n",
    "        datasets['test'],\n",
    "        batch_size=config['batch_size'],\n",
    "        shuffle=False,\n",
    "        num_workers=config['num_workers'],\n",
    "        pin_memory=config['pin_memory'],\n",
    "        drop_last=False  # Keep all data for evaluation\n",
    "    )\n",
    "    \n",
    "    # Print dataset information\n",
    "    print(\"=\" * 60)\n",
    "    print(\"DATA LOADERS CREATED\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for split, dataset in datasets.items():\n",
    "        print(f\"\\n{split.upper()} DATASET:\")\n",
    "        print(f\"  Total samples: {len(dataset):,}\")\n",
    "        print(f\"  Batches: {len(dataloaders[split]):,}\")\n",
    "        print(f\"  Batch size: {config['batch_size']}\")\n",
    "        \n",
    "        # Class distribution\n",
    "        for class_name, count in dataset.class_counts.items():\n",
    "            percentage = (count / len(dataset)) * 100\n",
    "            print(f\"  {class_name}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Calculate class weights for potential use\n",
    "    class_weights = datasets['train'].get_class_weights()\n",
    "    print(f\"\\nCLASS WEIGHTS (for balanced training):\")\n",
    "    for i, (class_name, weight) in enumerate(zip(CLASS_NAMES, class_weights)):\n",
    "        print(f\"  {class_name}: {weight:.3f}\")\n",
    "    \n",
    "    return dataloaders, datasets, class_weights\n",
    "\n",
    "# Create data loaders\n",
    "try:\n",
    "    dataloaders, datasets, class_weights = create_data_loaders(processed_data_dir, CONFIG)\n",
    "    print(f\"\\n✓ Data loaders created successfully\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error creating data loaders: {e}\")\n",
    "    raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8394fd9e",
   "metadata": {},
   "source": [
    "## 7. Model Architecture Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093c256e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientNetBinary(nn.Module):\n",
    "    \"\"\"EfficientNet-B0 for binary classification\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained=True):\n",
    "        super(EfficientNetBinary, self).__init__()\n",
    "        \n",
    "        if pretrained:\n",
    "            weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n",
    "            self.backbone = models.efficientnet_b0(weights=weights)\n",
    "        else:\n",
    "            self.backbone = models.efficientnet_b0(weights=None)\n",
    "        \n",
    "        # Replace classifier for binary classification\n",
    "        in_features = self.backbone.classifier[1].in_features\n",
    "        self.backbone.classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.2),\n",
    "            nn.Linear(in_features, 1)  # Single output for binary classification\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "class ResNetBinary(nn.Module):\n",
    "    \"\"\"ResNet-18 for binary classification\"\"\"\n",
    "    \n",
    "    def __init__(self, pretrained=True):\n",
    "        super(ResNetBinary, self).__init__()\n",
    "        \n",
    "        if pretrained:\n",
    "            weights = ResNet18_Weights.IMAGENET1K_V1\n",
    "            self.backbone = models.resnet18(weights=weights)\n",
    "        else:\n",
    "            self.backbone = models.resnet18(weights=None)\n",
    "        \n",
    "        # Replace final layer for binary classification\n",
    "        in_features = self.backbone.fc.in_features\n",
    "        self.backbone.fc = nn.Linear(in_features, 1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "class CustomCNN(nn.Module):\n",
    "    \"\"\"Custom CNN for binary classification (from scratch)\"\"\"\n",
    "    \n",
    "    def __init__(self, input_size=224, num_classes=1):\n",
    "        super(CustomCNN, self).__init__()\n",
    "        \n",
    "        # Feature extraction layers\n",
    "        self.features = nn.Sequential(\n",
    "            # Block 1\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(32, 32, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),  # 112x112\n",
    "            \n",
    "            # Block 2\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),  # 56x56\n",
    "            \n",
    "            # Block 3\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),  # 28x28\n",
    "            \n",
    "            # Block 4\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, stride=2),  # 14x14\n",
    "            \n",
    "            # Block 5\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1, 1))  # Global average pooling\n",
    "        )\n",
    "        \n",
    "        # Classifier\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def create_model(model_name, pretrained=True):\n",
    "    \"\"\"Create model based on configuration\"\"\"\n",
    "    \n",
    "    if model_name == 'efficientnet_b0':\n",
    "        model = EfficientNetBinary(pretrained=pretrained)\n",
    "    elif model_name == 'resnet18':\n",
    "        model = ResNetBinary(pretrained=pretrained)\n",
    "    elif model_name == 'custom_cnn':\n",
    "        model = CustomCNN()\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {model_name}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count trainable parameters in model\"\"\"\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    \n",
    "    return total_params, trainable_params\n",
    "\n",
    "# Create model\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL CREATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "model = create_model(CONFIG['model_name'], CONFIG['pretrained'])\n",
    "model = model.to(device)\n",
    "\n",
    "total_params, trainable_params = count_parameters(model)\n",
    "\n",
    "print(f\"Model: {CONFIG['model_name']}\")\n",
    "print(f\"Pretrained: {CONFIG['pretrained']}\")\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Model size: {total_params * 4 / 1024 / 1024:.1f} MB (float32)\")\n",
    "\n",
    "# Print model architecture summary\n",
    "print(f\"\\nModel architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c00d9c79",
   "metadata": {},
   "source": [
    "## 8. Training Setup (Loss, Optimizer, Scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cdc8447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "criterion = nn.BCEWithLogitsLoss()  # Includes sigmoid + BCE\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=CONFIG['learning_rate'],\n",
    "    weight_decay=CONFIG['weight_decay']\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "if CONFIG['scheduler_type'] == 'cosine':\n",
    "    scheduler = CosineAnnealingLR(\n",
    "        optimizer,\n",
    "        T_max=CONFIG['epochs'],\n",
    "        eta_min=CONFIG['learning_rate'] * 0.01\n",
    "    )\n",
    "elif CONFIG['scheduler_type'] == 'plateau':\n",
    "    scheduler = ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=CONFIG['plateau_factor'],\n",
    "        patience=CONFIG['plateau_patience'],\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "# Metrics tracking\n",
    "class MetricsTracker:\n",
    "    \"\"\"Track training and validation metrics\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "    \n",
    "    def reset(self):\n",
    "        self.metrics = {\n",
    "            'train_loss': [], 'train_acc': [], 'train_f1': [],\n",
    "            'val_loss': [], 'val_acc': [], 'val_f1': [], 'val_auc': [],\n",
    "            'learning_rates': []\n",
    "        }\n",
    "        self.best_metrics = {\n",
    "            'val_accuracy': 0.0, 'val_f1': 0.0, 'val_auc': 0.0,\n",
    "            'best_epoch': 0\n",
    "        }\n",
    "    \n",
    "    def update(self, epoch, train_metrics, val_metrics, lr):\n",
    "        \"\"\"Update metrics for current epoch\"\"\"\n",
    "        self.metrics['train_loss'].append(train_metrics['loss'])\n",
    "        self.metrics['train_acc'].append(train_metrics['accuracy'])\n",
    "        self.metrics['train_f1'].append(train_metrics['f1'])\n",
    "        \n",
    "        self.metrics['val_loss'].append(val_metrics['loss'])\n",
    "        self.metrics['val_acc'].append(val_metrics['accuracy'])\n",
    "        self.metrics['val_f1'].append(val_metrics['f1'])\n",
    "        self.metrics['val_auc'].append(val_metrics['auc'])\n",
    "        \n",
    "        self.metrics['learning_rates'].append(lr)\n",
    "        \n",
    "        # Update best metrics\n",
    "        if val_metrics['accuracy'] > self.best_metrics['val_accuracy']:\n",
    "            self.best_metrics['val_accuracy'] = val_metrics['accuracy']\n",
    "        if val_metrics['f1'] > self.best_metrics['val_f1']:\n",
    "            self.best_metrics['val_f1'] = val_metrics['f1']\n",
    "        if val_metrics['auc'] > self.best_metrics['val_auc']:\n",
    "            self.best_metrics['val_auc'] = val_metrics['auc']\n",
    "            self.best_metrics['best_epoch'] = epoch\n",
    "    \n",
    "    def get_best_metric(self, metric_name):\n",
    "        \"\"\"Get best value for specified metric\"\"\"\n",
    "        if metric_name == 'val_accuracy':\n",
    "            return self.best_metrics['val_accuracy']\n",
    "        elif metric_name == 'val_f1':\n",
    "            return self.best_metrics['val_f1']\n",
    "        elif metric_name == 'val_auc':\n",
    "            return self.best_metrics['val_auc']\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown metric: {metric_name}\")\n",
    "\n",
    "# Initialize metrics tracker\n",
    "metrics_tracker = MetricsTracker()\n",
    "\n",
    "# Early stopping\n",
    "class EarlyStopping:\n",
    "    \"\"\"Early stopping to prevent overfitting\"\"\"\n",
    "    \n",
    "    def __init__(self, patience=7, min_delta=0.001, mode='max'):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.mode = mode\n",
    "        self.counter = 0\n",
    "        self.best_score = None\n",
    "        self.early_stop = False\n",
    "        \n",
    "    def __call__(self, metric):\n",
    "        if self.mode == 'max':\n",
    "            score = metric\n",
    "            is_better = score > (self.best_score + self.min_delta) if self.best_score is not None else True\n",
    "        else:\n",
    "            score = -metric\n",
    "            is_better = score > (self.best_score + self.min_delta) if self.best_score is not None else True\n",
    "        \n",
    "        if is_better:\n",
    "            self.best_score = score\n",
    "            self.counter = 0\n",
    "        else:\n",
    "            self.counter += 1\n",
    "            \n",
    "        if self.counter >= self.patience:\n",
    "            self.early_stop = True\n",
    "        \n",
    "        return self.early_stop\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    patience=CONFIG['patience'],\n",
    "    mode='max'  # We want to maximize validation F1\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"TRAINING SETUP\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")\n",
    "print(f\"Scheduler: {CONFIG['scheduler_type']}\")\n",
    "print(f\"Early stopping patience: {CONFIG['patience']}\")\n",
    "print(f\"Best metric for saving: {CONFIG['save_best_metric']}\")\n",
    "\n",
    "print(f\"\\nInitial learning rate: {CONFIG['learning_rate']:.2e}\")\n",
    "print(f\"Weight decay: {CONFIG['weight_decay']:.2e}\")\n",
    "print(f\"Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"Max epochs: {CONFIG['epochs']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bdab81",
   "metadata": {},
   "source": [
    "## 9. Training and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e391fe39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_metrics(y_true, y_pred, y_prob):\n",
    "    \"\"\"Calculate comprehensive metrics\"\"\"\n",
    "    \n",
    "    # Convert to numpy\n",
    "    y_true = y_true.cpu().numpy()\n",
    "    y_pred = y_pred.cpu().numpy()\n",
    "    y_prob = y_prob.cpu().numpy()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='binary', zero_division=0\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        auc = roc_auc_score(y_true, y_prob)\n",
    "    except ValueError:\n",
    "        auc = 0.0  # Handle case where only one class is present\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device, print_freq=50):\n",
    "    \"\"\"Train for one epoch\"\"\"\n",
    "    model.train()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    pbar = tqdm(dataloader, desc=\"Training\")\n",
    "    \n",
    "    for batch_idx, (images, labels) in enumerate(pbar):\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images).squeeze()\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Statistics\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Get predictions\n",
    "        probabilities = torch.sigmoid(outputs)\n",
    "        predictions = (probabilities > 0.5).float()\n",
    "        \n",
    "        all_labels.extend(labels.cpu())\n",
    "        all_predictions.extend(predictions.cpu())\n",
    "        all_probabilities.extend(probabilities.cpu())\n",
    "        \n",
    "        # Update progress bar\n",
    "        if batch_idx % print_freq == 0:\n",
    "            pbar.set_postfix({\n",
    "                'Loss': f\"{running_loss/(batch_idx+1):.4f}\",\n",
    "                'LR': f\"{optimizer.param_groups[0]['lr']:.2e}\"\n",
    "            })\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    metrics = calculate_metrics(\n",
    "        torch.tensor(all_labels),\n",
    "        torch.tensor(all_predictions),\n",
    "        torch.tensor(all_probabilities)\n",
    "    )\n",
    "    metrics['loss'] = avg_loss\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "def validate_epoch(model, dataloader, criterion, device):\n",
    "    \"\"\"Validate for one epoch\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(dataloader, desc=\"Validating\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Statistics\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "            # Get predictions\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            \n",
    "            all_labels.extend(labels.cpu())\n",
    "            all_predictions.extend(predictions.cpu())\n",
    "            all_probabilities.extend(probabilities.cpu())\n",
    "    \n",
    "    # Calculate epoch metrics\n",
    "    avg_loss = running_loss / len(dataloader)\n",
    "    metrics = calculate_metrics(\n",
    "        torch.tensor(all_labels),\n",
    "        torch.tensor(all_predictions),\n",
    "        torch.tensor(all_probabilities)\n",
    "    )\n",
    "    metrics['loss'] = avg_loss\n",
    "    \n",
    "    return metrics, all_labels, all_predictions, all_probabilities\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, metrics, filepath):\n",
    "    \"\"\"Save model checkpoint\"\"\"\n",
    "    checkpoint = {\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'metrics': metrics,\n",
    "        'config': CONFIG\n",
    "    }\n",
    "    torch.save(checkpoint, filepath)\n",
    "\n",
    "def print_epoch_results(epoch, train_metrics, val_metrics, lr, best_metric):\n",
    "    \"\"\"Print formatted epoch results\"\"\"\n",
    "    print(f\"\\nEpoch {epoch+1:3d}/{CONFIG['epochs']}:\")\n",
    "    print(f\"  Train - Loss: {train_metrics['loss']:.4f}, \"\n",
    "          f\"Acc: {train_metrics['accuracy']:.4f}, \"\n",
    "          f\"F1: {train_metrics['f1']:.4f}\")\n",
    "    print(f\"  Val   - Loss: {val_metrics['loss']:.4f}, \"\n",
    "          f\"Acc: {val_metrics['accuracy']:.4f}, \"\n",
    "          f\"F1: {val_metrics['f1']:.4f}, \"\n",
    "          f\"AUC: {val_metrics['auc']:.4f}\")\n",
    "    print(f\"  LR: {lr:.2e}, Best {CONFIG['save_best_metric']}: {best_metric:.4f}\")\n",
    "\n",
    "print(\"✓ Training and evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91cd1f8e",
   "metadata": {},
   "source": [
    "## 10. Two-Stage Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "937fe6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_backbone(model, model_name):\n",
    "    \"\"\"Freeze backbone parameters for transfer learning\"\"\"\n",
    "    if model_name == 'efficientnet_b0':\n",
    "        # Freeze all parameters except classifier\n",
    "        for param in model.backbone.features.parameters():\n",
    "            param.requires_grad = False\n",
    "    elif model_name == 'resnet18':\n",
    "        # Freeze all parameters except final layer\n",
    "        for name, param in model.backbone.named_parameters():\n",
    "            if 'fc' not in name:\n",
    "                param.requires_grad = False\n",
    "    elif model_name == 'custom_cnn':\n",
    "        # Custom CNN trains from scratch\n",
    "        pass\n",
    "\n",
    "def unfreeze_backbone(model, model_name):\n",
    "    \"\"\"Unfreeze backbone parameters for fine-tuning\"\"\"\n",
    "    if model_name in ['efficientnet_b0', 'resnet18']:\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad = True\n",
    "\n",
    "def train_model():\n",
    "    \"\"\"Main training function with two-stage transfer learning\"\"\"\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"STARTING TRAINING\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Stage 1: Train only classifier head (if using pretrained model)\n",
    "    if CONFIG['pretrained'] and CONFIG['model_name'] != 'custom_cnn':\n",
    "        print(f\"\\n🔒 STAGE 1: Training classifier head only ({CONFIG['freeze_epochs']} epochs)\")\n",
    "        freeze_backbone(model, CONFIG['model_name'])\n",
    "        \n",
    "        # Show which parameters are trainable\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "        \n",
    "        # Train classifier head\n",
    "        for epoch in range(CONFIG['freeze_epochs']):\n",
    "            train_metrics = train_epoch(model, dataloaders['train'], criterion, optimizer, device)\n",
    "            val_metrics, _, _, _ = validate_epoch(model, dataloaders['val'], criterion, device)\n",
    "            \n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            best_metric = metrics_tracker.get_best_metric(CONFIG['save_best_metric'])\n",
    "            \n",
    "            # Update metrics\n",
    "            metrics_tracker.update(epoch, train_metrics, val_metrics, current_lr)\n",
    "            \n",
    "            # Print results\n",
    "            print_epoch_results(epoch, train_metrics, val_metrics, current_lr, best_metric)\n",
    "            \n",
    "            # Step scheduler\n",
    "            if CONFIG['scheduler_type'] == 'cosine':\n",
    "                scheduler.step()\n",
    "            elif CONFIG['scheduler_type'] == 'plateau':\n",
    "                scheduler.step(val_metrics['loss'])\n",
    "        \n",
    "        print(f\"✓ Stage 1 completed\")\n",
    "    \n",
    "    # Stage 2: Fine-tune entire model\n",
    "    print(f\"\\n🔓 STAGE 2: Fine-tuning entire model\")\n",
    "    \n",
    "    if CONFIG['pretrained'] and CONFIG['model_name'] != 'custom_cnn':\n",
    "        # Unfreeze backbone\n",
    "        unfreeze_backbone(model, CONFIG['model_name'])\n",
    "        \n",
    "        # Reduce learning rate for fine-tuning\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = CONFIG['learning_rate'] * CONFIG['finetune_lr_factor']\n",
    "        \n",
    "        print(f\"Reduced learning rate to: {optimizer.param_groups[0]['lr']:.2e}\")\n",
    "    \n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Continue training from where we left off\n",
    "    start_epoch = CONFIG['freeze_epochs'] if CONFIG['pretrained'] and CONFIG['model_name'] != 'custom_cnn' else 0\n",
    "    \n",
    "    best_model_path = models_dir / f\"best_{CONFIG['model_name']}_binary.pth\"\n",
    "    \n",
    "    for epoch in range(start_epoch, CONFIG['epochs']):\n",
    "        print(f\"\\n\" + \"=\"*50)\n",
    "        \n",
    "        # Train\n",
    "        train_metrics = train_epoch(model, dataloaders['train'], criterion, optimizer, device)\n",
    "        \n",
    "        # Validate\n",
    "        val_metrics, val_labels, val_predictions, val_probabilities = validate_epoch(\n",
    "            model, dataloaders['val'], criterion, device\n",
    "        )\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        \n",
    "        # Update metrics\n",
    "        metrics_tracker.update(epoch, train_metrics, val_metrics, current_lr)\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        current_metric = val_metrics[CONFIG['save_best_metric'].replace('val_', '')]\n",
    "        best_metric = metrics_tracker.get_best_metric(CONFIG['save_best_metric'])\n",
    "        \n",
    "        is_best = current_metric >= best_metric\n",
    "        \n",
    "        if is_best:\n",
    "            print(f\"🎉 New best {CONFIG['save_best_metric']}: {current_metric:.4f}\")\n",
    "            save_checkpoint(model, optimizer, epoch, val_metrics, best_model_path)\n",
    "        \n",
    "        # Print results\n",
    "        print_epoch_results(epoch, train_metrics, val_metrics, current_lr, best_metric)\n",
    "        \n",
    "        # Step scheduler\n",
    "        if CONFIG['scheduler_type'] == 'cosine':\n",
    "            scheduler.step()\n",
    "        elif CONFIG['scheduler_type'] == 'plateau':\n",
    "            scheduler.step(val_metrics['loss'])\n",
    "        \n",
    "        # Early stopping check\n",
    "        if early_stopping(current_metric):\n",
    "            print(f\"\\n⏹️  Early stopping triggered at epoch {epoch+1}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\n✅ Training completed!\")\n",
    "    print(f\"Best {CONFIG['save_best_metric']}: {metrics_tracker.get_best_metric(CONFIG['save_best_metric']):.4f}\")\n",
    "    print(f\"Best model saved: {best_model_path}\")\n",
    "    \n",
    "    return metrics_tracker\n",
    "\n",
    "# Start training\n",
    "try:\n",
    "    training_start_time = datetime.now()\n",
    "    print(f\"Training started at: {training_start_time}\")\n",
    "    \n",
    "    final_metrics = train_model()\n",
    "    \n",
    "    training_end_time = datetime.now()\n",
    "    training_duration = training_end_time - training_start_time\n",
    "    print(f\"\\nTraining completed at: {training_end_time}\")\n",
    "    print(f\"Total training time: {training_duration}\")\n",
    "    \n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\n⚠️  Training interrupted by user\")\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ Training failed with error: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e545a9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Restart training with fixed configuration (in case of previous issues)\n",
    "print(\"🔄 RESTARTING TRAINING WITH OPTIMIZED SETTINGS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Recreate data loaders with num_workers=0 to avoid deadlocks\n",
    "try:\n",
    "    print(\"Recreating data loaders with num_workers=0...\")\n",
    "    dataloaders, datasets, class_weights = create_data_loaders(processed_data_dir, CONFIG)\n",
    "    print(\"✅ Data loaders recreated successfully\")\n",
    "    \n",
    "    # Recreate model to ensure clean state\n",
    "    print(\"Recreating model...\")\n",
    "    model = create_model(CONFIG['model_name'], CONFIG['pretrained'])\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Recreate optimizer and scheduler\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG['weight_decay']\n",
    "    )\n",
    "    \n",
    "    if CONFIG['scheduler_type'] == 'cosine':\n",
    "        scheduler = CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=CONFIG['epochs'],\n",
    "            eta_min=CONFIG['learning_rate'] * 0.01\n",
    "        )\n",
    "    elif CONFIG['scheduler_type'] == 'plateau':\n",
    "        scheduler = ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='min',\n",
    "            factor=CONFIG['plateau_factor'],\n",
    "            patience=CONFIG['plateau_patience'],\n",
    "            verbose=True\n",
    "        )\n",
    "    \n",
    "    # Reset metrics and early stopping\n",
    "    metrics_tracker = MetricsTracker()\n",
    "    early_stopping = EarlyStopping(patience=CONFIG['patience'], mode='max')\n",
    "    \n",
    "    print(\"✅ Training setup recreated successfully\")\n",
    "    print(f\"Current num_workers: {CONFIG['num_workers']}\")\n",
    "    print(f\"Current batch_size: {CONFIG['batch_size']}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error recreating training setup: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd719d0c",
   "metadata": {},
   "source": [
    "## 11. Results Visualization and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673b6034",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_history(metrics_tracker, save_path=None):\n",
    "    \"\"\"Plot training history\"\"\"\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "    \n",
    "    epochs = range(1, len(metrics_tracker.metrics['train_loss']) + 1)\n",
    "    \n",
    "    # Loss plot\n",
    "    axes[0, 0].plot(epochs, metrics_tracker.metrics['train_loss'], 'b-', label='Train Loss')\n",
    "    axes[0, 0].plot(epochs, metrics_tracker.metrics['val_loss'], 'r-', label='Val Loss')\n",
    "    axes[0, 0].set_title('Training and Validation Loss')\n",
    "    axes[0, 0].set_xlabel('Epoch')\n",
    "    axes[0, 0].set_ylabel('Loss')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True)\n",
    "    \n",
    "    # Accuracy plot\n",
    "    axes[0, 1].plot(epochs, metrics_tracker.metrics['train_acc'], 'b-', label='Train Acc')\n",
    "    axes[0, 1].plot(epochs, metrics_tracker.metrics['val_acc'], 'r-', label='Val Acc')\n",
    "    axes[0, 1].set_title('Training and Validation Accuracy')\n",
    "    axes[0, 1].set_xlabel('Epoch')\n",
    "    axes[0, 1].set_ylabel('Accuracy')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True)\n",
    "    \n",
    "    # F1 Score plot\n",
    "    axes[1, 0].plot(epochs, metrics_tracker.metrics['train_f1'], 'b-', label='Train F1')\n",
    "    axes[1, 0].plot(epochs, metrics_tracker.metrics['val_f1'], 'r-', label='Val F1')\n",
    "    axes[1, 0].set_title('Training and Validation F1 Score')\n",
    "    axes[1, 0].set_xlabel('Epoch')\n",
    "    axes[1, 0].set_ylabel('F1 Score')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True)\n",
    "    \n",
    "    # Learning rate plot\n",
    "    axes[1, 1].plot(epochs, metrics_tracker.metrics['learning_rates'], 'g-')\n",
    "    axes[1, 1].set_title('Learning Rate Schedule')\n",
    "    axes[1, 1].set_xlabel('Epoch')\n",
    "    axes[1, 1].set_ylabel('Learning Rate')\n",
    "    axes[1, 1].set_yscale('log')\n",
    "    axes[1, 1].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "def evaluate_test_set(model, test_loader, device):\n",
    "    \"\"\"Comprehensive evaluation on test set\"\"\"\n",
    "    \n",
    "    # Load best model\n",
    "    best_model_path = models_dir / f\"best_{CONFIG['model_name']}_binary.pth\"\n",
    "    if best_model_path.exists():\n",
    "        checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"✓ Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader, desc=\"Testing\"):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            \n",
    "            all_labels.extend(labels.cpu())\n",
    "            all_predictions.extend(predictions.cpu())\n",
    "            all_probabilities.extend(probabilities.cpu())\n",
    "    \n",
    "    # Calculate metrics\n",
    "    test_metrics = calculate_metrics(\n",
    "        torch.tensor(all_labels),\n",
    "        torch.tensor(all_predictions),\n",
    "        torch.tensor(all_probabilities)\n",
    "    )\n",
    "    test_metrics['loss'] = test_loss / len(test_loader)\n",
    "    \n",
    "    return test_metrics, all_labels, all_predictions, all_probabilities\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, class_names, save_path=None):\n",
    "    \"\"\"Plot confusion matrix\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def plot_roc_curve(y_true, y_prob, save_path=None):\n",
    "    \"\"\"Plot ROC curve\"\"\"\n",
    "    \n",
    "    fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "    auc_score = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC Curve (AUC = {auc_score:.3f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Generate visualizations and final evaluation\n",
    "print(\"=\" * 70)\n",
    "print(\"FINAL EVALUATION AND VISUALIZATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Plot training history\n",
    "if 'final_metrics' in locals():\n",
    "    print(\"\\n📈 Plotting training history...\")\n",
    "    plot_training_history(\n",
    "        final_metrics, \n",
    "        save_path=outputs_dir / f\"training_history_{CONFIG['model_name']}.png\"\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\n🧪 Evaluating on test set...\")\n",
    "    test_metrics, test_labels, test_predictions, test_probabilities = evaluate_test_set(\n",
    "        model, dataloaders['test'], device\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTEST SET RESULTS:\")\n",
    "    print(f\"  Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {test_metrics['precision']:.4f}\")\n",
    "    print(f\"  Recall: {test_metrics['recall']:.4f}\")\n",
    "    print(f\"  F1 Score: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"  AUC: {test_metrics['auc']:.4f}\")\n",
    "    print(f\"  Loss: {test_metrics['loss']:.4f}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    print(f\"\\n📊 Generating confusion matrix...\")\n",
    "    cm = plot_confusion_matrix(\n",
    "        test_labels, test_predictions, CLASS_NAMES,\n",
    "        save_path=outputs_dir / f\"confusion_matrix_{CONFIG['model_name']}.png\"\n",
    "    )\n",
    "    \n",
    "    # Plot ROC curve\n",
    "    print(f\"\\n📈 Generating ROC curve...\")\n",
    "    plot_roc_curve(\n",
    "        test_labels, test_probabilities,\n",
    "        save_path=outputs_dir / f\"roc_curve_{CONFIG['model_name']}.png\"\n",
    "    )\n",
    "    \n",
    "    # Generate detailed classification report\n",
    "    print(f\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(test_labels, test_predictions, target_names=CLASS_NAMES))\n",
    "    \n",
    "    # Save final results\n",
    "    results = {\n",
    "        'model_name': CONFIG['model_name'],\n",
    "        'training_config': CONFIG,\n",
    "        'test_metrics': test_metrics,\n",
    "        'best_validation_metrics': {\n",
    "            'val_accuracy': final_metrics.get_best_metric('val_accuracy'),\n",
    "            'val_f1': final_metrics.get_best_metric('val_f1'),\n",
    "            'val_auc': final_metrics.get_best_metric('val_auc'),\n",
    "            'best_epoch': final_metrics.best_metrics['best_epoch']\n",
    "        },\n",
    "        'timestamp': datetime.now().isoformat()\n",
    "    }\n",
    "    \n",
    "    results_path = outputs_dir / f\"final_results_{CONFIG['model_name']}.json\"\n",
    "    with open(results_path, 'w') as f:\n",
    "        json.dump(results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n💾 Results saved to: {results_path}\")\n",
    "    \n",
    "    # Check if we achieved target accuracy\n",
    "    target_accuracy = 0.95\n",
    "    achieved_target = test_metrics['accuracy'] >= target_accuracy\n",
    "    \n",
    "    print(f\"\\n🎯 TARGET EVALUATION:\")\n",
    "    print(f\"  Target accuracy: {target_accuracy:.1%}\")\n",
    "    print(f\"  Achieved accuracy: {test_metrics['accuracy']:.1%}\")\n",
    "    print(f\"  Target achieved: {'✅ YES' if achieved_target else '❌ NO'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Training metrics not available for visualization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab0541",
   "metadata": {},
   "source": [
    "## 12. Model Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeb2b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 70)\n",
    "print(\"TRAINING COMPLETE - SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Print final summary\n",
    "print(f\"\\n🏁 TRAINING SUMMARY:\")\n",
    "print(f\"  Model: {CONFIG['model_name']}\")\n",
    "print(f\"  Pretrained: {CONFIG['pretrained']}\")\n",
    "print(f\"  Image size: {CONFIG['image_size']}x{CONFIG['image_size']}\")\n",
    "print(f\"  Batch size: {CONFIG['batch_size']}\")\n",
    "print(f\"  Total epochs: {CONFIG['epochs']}\")\n",
    "\n",
    "if 'final_metrics' in locals() and 'test_metrics' in locals():\n",
    "    print(f\"\\n📊 BEST RESULTS:\")\n",
    "    print(f\"  Validation Accuracy: {final_metrics.get_best_metric('val_accuracy'):.4f}\")\n",
    "    print(f\"  Validation F1: {final_metrics.get_best_metric('val_f1'):.4f}\")\n",
    "    print(f\"  Validation AUC: {final_metrics.get_best_metric('val_auc'):.4f}\")\n",
    "    \n",
    "    print(f\"\\n🧪 TEST SET PERFORMANCE:\")\n",
    "    print(f\"  Test Accuracy: {test_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Test F1: {test_metrics['f1']:.4f}\")\n",
    "    print(f\"  Test AUC: {test_metrics['auc']:.4f}\")\n",
    "    \n",
    "    target_met = test_metrics['accuracy'] >= 0.95\n",
    "    print(f\"\\n🎯 TARGET ACHIEVEMENT:\")\n",
    "    print(f\"  95% Accuracy Target: {'✅ ACHIEVED' if target_met else '❌ NOT ACHIEVED'}\")\n",
    "    print(f\"  Actual Performance: {test_metrics['accuracy']:.1%}\")\n",
    "\n",
    "print(f\"\\n📁 SAVED ARTIFACTS:\")\n",
    "print(f\"  Best model: models/best_{CONFIG['model_name']}_binary.pth\")\n",
    "print(f\"  Training plots: outputs/training_history_{CONFIG['model_name']}.png\")\n",
    "print(f\"  Confusion matrix: outputs/confusion_matrix_{CONFIG['model_name']}.png\")\n",
    "print(f\"  ROC curve: outputs/roc_curve_{CONFIG['model_name']}.png\")\n",
    "print(f\"  Final results: outputs/final_results_{CONFIG['model_name']}.json\")\n",
    "\n",
    "print(f\"\\n🚀 NEXT STEPS:\")\n",
    "print(f\"  1. Experiment with other models (ResNet18, Custom CNN)\")\n",
    "print(f\"  2. Hyperparameter tuning (learning rate, batch size, augmentations)\")\n",
    "print(f\"  3. Cross-validation for robust evaluation\")\n",
    "print(f\"  4. Error analysis on misclassified samples\")\n",
    "print(f\"  5. Ensemble methods for improved performance\")\n",
    "print(f\"  6. Deploy model for inference\")\n",
    "\n",
    "print(f\"\\n💡 OPTIMIZATION SUGGESTIONS:\")\n",
    "if 'test_metrics' in locals():\n",
    "    if test_metrics['accuracy'] < 0.95:\n",
    "        print(f\"  - Try larger models (EfficientNet-B1/B2)\")\n",
    "        print(f\"  - Increase training epochs with patience\")\n",
    "        print(f\"  - Experiment with different augmentation strategies\")\n",
    "        print(f\"  - Use ensemble of multiple models\")\n",
    "        print(f\"  - Fine-tune with different learning rates\")\n",
    "    else:\n",
    "        print(f\"  - Model performance is excellent!\")\n",
    "        print(f\"  - Consider model compression for deployment\")\n",
    "        print(f\"  - Validate on additional test data\")\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 70)\n",
    "print(\"BINARY CLASSIFICATION TRAINING COMPLETED!\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323cdbf3",
   "metadata": {},
   "source": [
    "## 13. Comprehensive Evaluation & Reporting\n",
    "\n",
    "This section provides detailed evaluation metrics, visualizations, and error analysis on the held-out test set following best practices for model evaluation and reproducibility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab174c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Test Set Evaluation\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_recall_fscore_support, confusion_matrix,\n",
    "    classification_report, roc_auc_score, roc_curve, average_precision_score,\n",
    "    precision_recall_curve\n",
    ")\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "def comprehensive_evaluation(model, test_loader, device, class_names):\n",
    "    \"\"\"\n",
    "    Comprehensive evaluation on test set with detailed metrics and visualizations\n",
    "    \"\"\"\n",
    "    print(\"=\" * 80)\n",
    "    print(\"COMPREHENSIVE TEST SET EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Create reports directory\n",
    "    reports_dir = project_root / 'reports'\n",
    "    reports_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Load best model\n",
    "    best_model_path = models_dir / f\"best_{CONFIG['model_name']}_binary.pth\"\n",
    "    if best_model_path.exists():\n",
    "        checkpoint = torch.load(best_model_path, map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(f\"✓ Loaded best model from epoch {checkpoint['epoch']+1}\")\n",
    "        best_epoch = checkpoint['epoch'] + 1\n",
    "    else:\n",
    "        print(\"⚠️  Best model not found, using current model state\")\n",
    "        best_epoch = \"Unknown\"\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Collect predictions and true labels\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "    all_probabilities = []\n",
    "    all_image_paths = []\n",
    "    test_loss = 0.0\n",
    "    \n",
    "    print(\"\\n🔍 Running inference on test set...\")\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, labels) in enumerate(tqdm(test_loader, desc=\"Testing\")):\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            # Get image paths for this batch (for error analysis)\n",
    "            start_idx = batch_idx * test_loader.batch_size\n",
    "            end_idx = start_idx + len(labels)\n",
    "            batch_paths = [test_loader.dataset.samples[i][0] for i in range(start_idx, min(end_idx, len(test_loader.dataset)))]\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images).squeeze()\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            \n",
    "            # Get predictions and probabilities\n",
    "            probabilities = torch.sigmoid(outputs)\n",
    "            predictions = (probabilities > 0.5).float()\n",
    "            \n",
    "            # Store results\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_probabilities.extend(probabilities.cpu().numpy())\n",
    "            all_image_paths.extend(batch_paths)\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_predictions)\n",
    "    y_prob = np.array(all_probabilities)\n",
    "    \n",
    "    print(f\"✓ Inference completed on {len(y_true)} test samples\")\n",
    "    \n",
    "    return y_true, y_pred, y_prob, all_image_paths, test_loss / len(test_loader), best_epoch\n",
    "\n",
    "def calculate_detailed_metrics(y_true, y_pred, y_prob, class_names):\n",
    "    \"\"\"Calculate comprehensive metrics including macro and per-class statistics\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Per-class metrics (binary classification)\n",
    "    precision_per_class, recall_per_class, f1_per_class, support_per_class = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average=None, zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Macro-averaged metrics\n",
    "    precision_macro, recall_macro, f1_macro, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='macro', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # Weighted metrics (account for class imbalance)\n",
    "    precision_weighted, recall_weighted, f1_weighted, _ = precision_recall_fscore_support(\n",
    "        y_true, y_pred, average='weighted', zero_division=0\n",
    "    )\n",
    "    \n",
    "    # ROC-AUC and PR-AUC\n",
    "    try:\n",
    "        roc_auc = roc_auc_score(y_true, y_prob)\n",
    "        pr_auc = average_precision_score(y_true, y_prob)\n",
    "    except ValueError:\n",
    "        roc_auc = 0.0\n",
    "        pr_auc = 0.0\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # True negatives, false positives, false negatives, true positives\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Specificity (True Negative Rate)\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0.0\n",
    "    \n",
    "    # Sensitivity (True Positive Rate / Recall)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else 0.0\n",
    "    \n",
    "    metrics = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision_macro': precision_macro,\n",
    "        'recall_macro': recall_macro,\n",
    "        'f1_macro': f1_macro,\n",
    "        'precision_weighted': precision_weighted,\n",
    "        'recall_weighted': recall_weighted,\n",
    "        'f1_weighted': f1_weighted,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'specificity': specificity,\n",
    "        'sensitivity': sensitivity,\n",
    "        'confusion_matrix': cm.tolist(),\n",
    "        'per_class_metrics': {\n",
    "            class_names[i]: {\n",
    "                'precision': precision_per_class[i],\n",
    "                'recall': recall_per_class[i],\n",
    "                'f1': f1_per_class[i],\n",
    "                'support': int(support_per_class[i])\n",
    "            } for i in range(len(class_names))\n",
    "        },\n",
    "        'raw_counts': {\n",
    "            'true_negatives': int(tn),\n",
    "            'false_positives': int(fp),\n",
    "            'false_negatives': int(fn),\n",
    "            'true_positives': int(tp)\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run comprehensive evaluation\n",
    "if 'model' in locals() and 'dataloaders' in locals():\n",
    "    print(\"Starting comprehensive evaluation...\")\n",
    "    \n",
    "    y_true, y_pred, y_prob, image_paths, test_loss, best_epoch = comprehensive_evaluation(\n",
    "        model, dataloaders['test'], device, CLASS_NAMES\n",
    "    )\n",
    "    \n",
    "    # Calculate detailed metrics\n",
    "    detailed_metrics = calculate_detailed_metrics(y_true, y_pred, y_prob, CLASS_NAMES)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DETAILED TEST SET METRICS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    print(f\"\\n📊 OVERALL PERFORMANCE:\")\n",
    "    print(f\"  Accuracy: {detailed_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📈 MACRO-AVERAGED METRICS:\")\n",
    "    print(f\"  Precision: {detailed_metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall: {detailed_metrics['recall_macro']:.4f}\")\n",
    "    print(f\"  F1-Score: {detailed_metrics['f1_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n⚖️  WEIGHTED METRICS (Class-Balanced):\")\n",
    "    print(f\"  Precision: {detailed_metrics['precision_weighted']:.4f}\")\n",
    "    print(f\"  Recall: {detailed_metrics['recall_weighted']:.4f}\")\n",
    "    print(f\"  F1-Score: {detailed_metrics['f1_weighted']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📋 PER-CLASS METRICS:\")\n",
    "    for class_name, metrics in detailed_metrics['per_class_metrics'].items():\n",
    "        print(f\"  {class_name}:\")\n",
    "        print(f\"    Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"    Recall: {metrics['recall']:.4f}\")\n",
    "        print(f\"    F1-Score: {metrics['f1']:.4f}\")\n",
    "        print(f\"    Support: {metrics['support']} samples\")\n",
    "    \n",
    "    print(f\"\\n🎯 ADDITIONAL METRICS:\")\n",
    "    print(f\"  ROC-AUC: {detailed_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  PR-AUC: {detailed_metrics['pr_auc']:.4f}\")\n",
    "    print(f\"  Sensitivity (Recall): {detailed_metrics['sensitivity']:.4f}\")\n",
    "    print(f\"  Specificity: {detailed_metrics['specificity']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n🔢 CONFUSION MATRIX COUNTS:\")\n",
    "    print(f\"  True Negatives: {detailed_metrics['raw_counts']['true_negatives']}\")\n",
    "    print(f\"  False Positives: {detailed_metrics['raw_counts']['false_positives']}\")\n",
    "    print(f\"  False Negatives: {detailed_metrics['raw_counts']['false_negatives']}\")\n",
    "    print(f\"  True Positives: {detailed_metrics['raw_counts']['true_positives']}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Model or data loaders not available for evaluation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26d38cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization Functions\n",
    "def plot_detailed_confusion_matrix(y_true, y_pred, class_names, save_path=None):\n",
    "    \"\"\"Plot detailed confusion matrix with percentages and counts\"\"\"\n",
    "    \n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    cm_normalized = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Raw counts\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax1)\n",
    "    ax1.set_title('Confusion Matrix (Counts)')\n",
    "    ax1.set_xlabel('Predicted Label')\n",
    "    ax1.set_ylabel('True Label')\n",
    "    \n",
    "    # Percentages\n",
    "    sns.heatmap(cm_normalized, annot=True, fmt='.2%', cmap='Blues',\n",
    "                xticklabels=class_names, yticklabels=class_names, ax=ax2)\n",
    "    ax2.set_title('Confusion Matrix (Percentages)')\n",
    "    ax2.set_xlabel('Predicted Label')\n",
    "    ax2.set_ylabel('True Label')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ Confusion matrix saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return cm\n",
    "\n",
    "def plot_roc_and_pr_curves(y_true, y_prob, save_path=None):\n",
    "    \"\"\"Plot ROC and Precision-Recall curves\"\"\"\n",
    "    \n",
    "    # Calculate curves\n",
    "    fpr, tpr, roc_thresholds = roc_curve(y_true, y_prob)\n",
    "    roc_auc = roc_auc_score(y_true, y_prob)\n",
    "    \n",
    "    precision, recall, pr_thresholds = precision_recall_curve(y_true, y_prob)\n",
    "    pr_auc = average_precision_score(y_true, y_prob)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # ROC Curve\n",
    "    ax1.plot(fpr, tpr, color='darkorange', lw=2, \n",
    "             label=f'ROC Curve (AUC = {roc_auc:.3f})')\n",
    "    ax1.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--', label='Random Classifier')\n",
    "    ax1.set_xlim([0.0, 1.0])\n",
    "    ax1.set_ylim([0.0, 1.05])\n",
    "    ax1.set_xlabel('False Positive Rate')\n",
    "    ax1.set_ylabel('True Positive Rate')\n",
    "    ax1.set_title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    ax1.legend(loc=\"lower right\")\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Precision-Recall Curve\n",
    "    ax2.plot(recall, precision, color='darkgreen', lw=2,\n",
    "             label=f'PR Curve (AUC = {pr_auc:.3f})')\n",
    "    ax2.axhline(y=np.mean(y_true), color='navy', linestyle='--', \n",
    "                label=f'Baseline (Prevalence = {np.mean(y_true):.3f})')\n",
    "    ax2.set_xlim([0.0, 1.0])\n",
    "    ax2.set_ylim([0.0, 1.05])\n",
    "    ax2.set_xlabel('Recall')\n",
    "    ax2.set_ylabel('Precision')\n",
    "    ax2.set_title('Precision-Recall Curve')\n",
    "    ax2.legend(loc=\"lower left\")\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"✓ ROC and PR curves saved to: {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'fpr': fpr.tolist(),\n",
    "        'tpr': tpr.tolist(),\n",
    "        'roc_thresholds': roc_thresholds.tolist(),\n",
    "        'precision': precision.tolist(),\n",
    "        'recall': recall.tolist(),\n",
    "        'pr_thresholds': pr_thresholds.tolist()\n",
    "    }\n",
    "\n",
    "# Generate detailed visualizations\n",
    "if 'y_true' in locals():\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GENERATING DETAILED VISUALIZATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    reports_dir = project_root / 'reports'\n",
    "    reports_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # 1. Detailed Confusion Matrix\n",
    "    print(\"\\n📊 Generating detailed confusion matrix...\")\n",
    "    cm = plot_detailed_confusion_matrix(\n",
    "        y_true, y_pred, CLASS_NAMES,\n",
    "        save_path=reports_dir / f\"confusion_matrix_detailed_{CONFIG['model_name']}.png\"\n",
    "    )\n",
    "    \n",
    "    # 2. ROC and PR Curves\n",
    "    print(\"\\n📈 Generating ROC and Precision-Recall curves...\")\n",
    "    curve_data = plot_roc_and_pr_curves(\n",
    "        y_true, y_prob,\n",
    "        save_path=reports_dir / f\"roc_pr_curves_{CONFIG['model_name']}.png\"\n",
    "    )\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Evaluation results not available for visualization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f017a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Error Analysis and Sample Visualization\n",
    "def analyze_errors(y_true, y_pred, y_prob, image_paths, class_names, top_n=5):\n",
    "    \"\"\"Analyze false positives and false negatives\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ERROR ANALYSIS\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create error analysis DataFrame\n",
    "    error_df = pd.DataFrame({\n",
    "        'image_path': image_paths,\n",
    "        'true_label': y_true,\n",
    "        'pred_label': y_pred,\n",
    "        'probability': y_prob,\n",
    "        'true_class': [class_names[int(label)] for label in y_true],\n",
    "        'pred_class': [class_names[int(label)] for label in y_pred]\n",
    "    })\n",
    "    \n",
    "    # Add error columns\n",
    "    error_df['correct'] = error_df['true_label'] == error_df['pred_label']\n",
    "    error_df['confidence'] = np.maximum(error_df['probability'], 1 - error_df['probability'])\n",
    "    \n",
    "    # False Positives (predicted 1, actually 0)\n",
    "    false_positives = error_df[(error_df['true_label'] == 0) & (error_df['pred_label'] == 1)]\n",
    "    false_positives = false_positives.sort_values('probability', ascending=False)\n",
    "    \n",
    "    # False Negatives (predicted 0, actually 1)\n",
    "    false_negatives = error_df[(error_df['true_label'] == 1) & (error_df['pred_label'] == 0)]\n",
    "    false_negatives = false_negatives.sort_values('probability', ascending=True)\n",
    "    \n",
    "    # True Positives (high confidence correct predictions of class 1)\n",
    "    true_positives = error_df[(error_df['true_label'] == 1) & (error_df['pred_label'] == 1)]\n",
    "    true_positives = true_positives.sort_values('probability', ascending=False)\n",
    "    \n",
    "    # True Negatives (high confidence correct predictions of class 0)\n",
    "    true_negatives = error_df[(error_df['true_label'] == 0) & (error_df['pred_label'] == 0)]\n",
    "    true_negatives = true_negatives.sort_values('probability', ascending=True)\n",
    "    \n",
    "    print(f\"\\n📊 ERROR SUMMARY:\")\n",
    "    print(f\"  Total samples: {len(error_df)}\")\n",
    "    print(f\"  Correct predictions: {len(error_df[error_df['correct']])}\")\n",
    "    print(f\"  Incorrect predictions: {len(error_df[~error_df['correct']])}\")\n",
    "    print(f\"  False Positives: {len(false_positives)}\")\n",
    "    print(f\"  False Negatives: {len(false_negatives)}\")\n",
    "    \n",
    "    if len(false_positives) > 0:\n",
    "        print(f\"\\n❌ TOP {min(top_n, len(false_positives))} FALSE POSITIVES:\")\n",
    "        print(f\"   (Predicted {class_names[1]}, Actually {class_names[0]})\")\n",
    "        for i, (_, row) in enumerate(false_positives.head(top_n).iterrows()):\n",
    "            print(f\"   {i+1}. Confidence: {row['probability']:.3f} | {Path(row['image_path']).name}\")\n",
    "    \n",
    "    if len(false_negatives) > 0:\n",
    "        print(f\"\\n❌ TOP {min(top_n, len(false_negatives))} FALSE NEGATIVES:\")\n",
    "        print(f\"   (Predicted {class_names[0]}, Actually {class_names[1]})\")\n",
    "        for i, (_, row) in enumerate(false_negatives.head(top_n).iterrows()):\n",
    "            print(f\"   {i+1}. Confidence: {1-row['probability']:.3f} | {Path(row['image_path']).name}\")\n",
    "    \n",
    "    return {\n",
    "        'error_df': error_df,\n",
    "        'false_positives': false_positives,\n",
    "        'false_negatives': false_negatives,\n",
    "        'true_positives': true_positives,\n",
    "        'true_negatives': true_negatives\n",
    "    }\n",
    "\n",
    "def visualize_error_samples(error_data, sample_type='false_positives', n_samples=6, figsize=(15, 10)):\n",
    "    \"\"\"Visualize error samples\"\"\"\n",
    "    \n",
    "    if sample_type not in error_data:\n",
    "        print(f\"⚠️  {sample_type} not found in error data\")\n",
    "        return\n",
    "    \n",
    "    samples_df = error_data[sample_type].head(n_samples)\n",
    "    \n",
    "    if len(samples_df) == 0:\n",
    "        print(f\"✅ No {sample_type} found!\")\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n🖼️  Visualizing top {len(samples_df)} {sample_type}...\")\n",
    "    \n",
    "    # Calculate grid dimensions\n",
    "    cols = min(3, len(samples_df))\n",
    "    rows = (len(samples_df) + cols - 1) // cols\n",
    "    \n",
    "    fig, axes = plt.subplots(rows, cols, figsize=figsize)\n",
    "    if rows == 1 and cols == 1:\n",
    "        axes = [axes]\n",
    "    elif rows == 1:\n",
    "        axes = axes\n",
    "    else:\n",
    "        axes = axes.flatten()\n",
    "    \n",
    "    for i, (_, row) in enumerate(samples_df.iterrows()):\n",
    "        if i >= len(axes):\n",
    "            break\n",
    "            \n",
    "        # Load and display image\n",
    "        try:\n",
    "            img = Image.open(row['image_path'])\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].axis('off')\n",
    "            \n",
    "            # Create title with error information\n",
    "            title = f\"True: {row['true_class']}\\nPred: {row['pred_class']}\\nConf: {row['probability']:.3f}\"\n",
    "            axes[i].set_title(title, fontsize=10)\n",
    "            \n",
    "        except Exception as e:\n",
    "            axes[i].text(0.5, 0.5, f\"Error loading\\n{Path(row['image_path']).name}\", \n",
    "                        ha='center', va='center', transform=axes[i].transAxes)\n",
    "            axes[i].set_title(f\"Conf: {row['probability']:.3f}\")\n",
    "    \n",
    "    # Hide extra subplots\n",
    "    for i in range(len(samples_df), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "    \n",
    "    plt.suptitle(f\"Top {sample_type.replace('_', ' ').title()}\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Run error analysis\n",
    "if 'y_true' in locals():\n",
    "    error_analysis = analyze_errors(y_true, y_pred, y_prob, image_paths, CLASS_NAMES, top_n=5)\n",
    "    \n",
    "    # Visualize error samples (you can run these individually)\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"SAMPLE VISUALIZATIONS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Note: Run the visualization functions below to see sample images\")\n",
    "    \n",
    "    # Store visualization commands for user to run\n",
    "    viz_commands = [\n",
    "        \"# Visualize False Positives\",\n",
    "        \"visualize_error_samples(error_analysis, 'false_positives', n_samples=6)\",\n",
    "        \"\",\n",
    "        \"# Visualize False Negatives\", \n",
    "        \"visualize_error_samples(error_analysis, 'false_negatives', n_samples=6)\",\n",
    "        \"\",\n",
    "        \"# Visualize High-Confidence True Positives\",\n",
    "        \"visualize_error_samples(error_analysis, 'true_positives', n_samples=6)\",\n",
    "        \"\",\n",
    "        \"# Visualize High-Confidence True Negatives\",\n",
    "        \"visualize_error_samples(error_analysis, 'true_negatives', n_samples=6)\"\n",
    "    ]\n",
    "    \n",
    "    print(\"\\n📋 Available visualization commands:\")\n",
    "    for cmd in viz_commands:\n",
    "        if cmd.startswith(\"#\"):\n",
    "            print(f\"\\n{cmd}\")\n",
    "        elif cmd:\n",
    "            print(f\"   {cmd}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Evaluation results not available for error analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7ebd68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset Documentation and Final Report Generation\n",
    "def document_dataset_splits():\n",
    "    \"\"\"Document exact data counts per split and per class for reproducibility\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"DATASET DOCUMENTATION\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Collect detailed dataset statistics\n",
    "    dataset_documentation = {\n",
    "        'metadata': {\n",
    "            'creation_date': datetime.now().isoformat(),\n",
    "            'model_name': CONFIG['model_name'],\n",
    "            'image_size': CONFIG['image_size'],\n",
    "            'preprocessing': 'ImageNet normalization, 3-channel conversion',\n",
    "            'class_names': CLASS_NAMES,\n",
    "            'class_mapping': CLASS_TO_IDX\n",
    "        },\n",
    "        'splits': {}\n",
    "    }\n",
    "    \n",
    "    total_images = 0\n",
    "    splits = ['train', 'val', 'test']\n",
    "    \n",
    "    print(f\"\\n📁 DATASET SPLITS DOCUMENTATION:\")\n",
    "    \n",
    "    for split in splits:\n",
    "        split_dir = processed_data_dir / split\n",
    "        split_info = {\n",
    "            'classes': {},\n",
    "            'total': 0,\n",
    "            'path': str(split_dir)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n{split.upper()} SET:\")\n",
    "        \n",
    "        for class_name in CLASS_NAMES:\n",
    "            class_dir = split_dir / class_name\n",
    "            \n",
    "            if class_dir.exists():\n",
    "                # Count all image files\n",
    "                image_extensions = ['*.jpg', '*.jpeg', '*.png', '*.tiff', '*.tif', '*.bmp']\n",
    "                image_files = []\n",
    "                for ext in image_extensions:\n",
    "                    image_files.extend(list(class_dir.glob(ext)))\n",
    "                \n",
    "                count = len(image_files)\n",
    "                split_info['classes'][class_name] = {\n",
    "                    'count': count,\n",
    "                    'path': str(class_dir),\n",
    "                    'percentage': 0  # Will calculate after total\n",
    "                }\n",
    "                split_info['total'] += count\n",
    "                total_images += count\n",
    "                \n",
    "                print(f\"  {class_name}: {count:,} images\")\n",
    "            else:\n",
    "                print(f\"  {class_name}: Directory not found - {class_dir}\")\n",
    "                split_info['classes'][class_name] = {\n",
    "                    'count': 0,\n",
    "                    'path': str(class_dir),\n",
    "                    'percentage': 0\n",
    "                }\n",
    "        \n",
    "        # Calculate percentages within split\n",
    "        for class_name in CLASS_NAMES:\n",
    "            if split_info['total'] > 0:\n",
    "                split_info['classes'][class_name]['percentage'] = \\\n",
    "                    (split_info['classes'][class_name]['count'] / split_info['total']) * 100\n",
    "        \n",
    "        dataset_documentation['splits'][split] = split_info\n",
    "        print(f\"  Total {split}: {split_info['total']:,} images\")\n",
    "        \n",
    "        # Show class distribution\n",
    "        for class_name in CLASS_NAMES:\n",
    "            percentage = split_info['classes'][class_name]['percentage']\n",
    "            print(f\"    {class_name}: {percentage:.1f}%\")\n",
    "    \n",
    "    # Calculate overall statistics\n",
    "    dataset_documentation['metadata']['total_images'] = total_images\n",
    "    \n",
    "    print(f\"\\n📊 OVERALL STATISTICS:\")\n",
    "    print(f\"  Total images across all splits: {total_images:,}\")\n",
    "    \n",
    "    # Calculate split percentages\n",
    "    for split in splits:\n",
    "        split_total = dataset_documentation['splits'][split]['total']\n",
    "        split_percentage = (split_total / total_images * 100) if total_images > 0 else 0\n",
    "        dataset_documentation['splits'][split]['split_percentage'] = split_percentage\n",
    "        print(f\"  {split}: {split_total:,} images ({split_percentage:.1f}%)\")\n",
    "    \n",
    "    # Overall class distribution\n",
    "    print(f\"\\n📈 OVERALL CLASS DISTRIBUTION:\")\n",
    "    overall_class_counts = {}\n",
    "    for class_name in CLASS_NAMES:\n",
    "        total_class_count = sum(\n",
    "            dataset_documentation['splits'][split]['classes'][class_name]['count'] \n",
    "            for split in splits\n",
    "        )\n",
    "        overall_class_counts[class_name] = total_class_count\n",
    "        percentage = (total_class_count / total_images * 100) if total_images > 0 else 0\n",
    "        print(f\"  {class_name}: {total_class_count:,} images ({percentage:.1f}%)\")\n",
    "    \n",
    "    dataset_documentation['metadata']['overall_class_distribution'] = overall_class_counts\n",
    "    \n",
    "    return dataset_documentation\n",
    "\n",
    "def generate_final_report(detailed_metrics, curve_data, dataset_doc, error_analysis, best_epoch):\n",
    "    \"\"\"Generate comprehensive final report with all metrics and documentation\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"GENERATING FINAL REPORT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Create comprehensive report\n",
    "    final_report = {\n",
    "        'experiment_info': {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'model_name': CONFIG['model_name'],\n",
    "            'best_epoch': best_epoch,\n",
    "            'training_config': CONFIG,\n",
    "            'target_accuracy': 0.95,\n",
    "            'target_achieved': detailed_metrics['accuracy'] >= 0.95\n",
    "        },\n",
    "        'dataset_documentation': dataset_doc,\n",
    "        'test_metrics': {\n",
    "            **detailed_metrics,\n",
    "            'test_loss': test_loss,\n",
    "            'curve_metrics': {\n",
    "                'roc_auc': curve_data['roc_auc'],\n",
    "                'pr_auc': curve_data['pr_auc']\n",
    "            }\n",
    "        },\n",
    "        'error_analysis_summary': {\n",
    "            'total_samples': len(error_analysis['error_df']),\n",
    "            'correct_predictions': len(error_analysis['error_df'][error_analysis['error_df']['correct']]),\n",
    "            'false_positives_count': len(error_analysis['false_positives']),\n",
    "            'false_negatives_count': len(error_analysis['false_negatives']),\n",
    "            'top_false_positives': error_analysis['false_positives'].head(5)[['image_path', 'probability', 'true_class', 'pred_class']].to_dict('records'),\n",
    "            'top_false_negatives': error_analysis['false_negatives'].head(5)[['image_path', 'probability', 'true_class', 'pred_class']].to_dict('records')\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # Save JSON report\n",
    "    reports_dir = project_root / 'reports'\n",
    "    reports_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    json_path = reports_dir / f\"final_report_{CONFIG['model_name']}.json\"\n",
    "    with open(json_path, 'w') as f:\n",
    "        json.dump(final_report, f, indent=2, default=str)\n",
    "    print(f\"✓ JSON report saved to: {json_path}\")\n",
    "    \n",
    "    # Save CSV summary\n",
    "    csv_data = {\n",
    "        'Metric': [],\n",
    "        'Value': []\n",
    "    }\n",
    "    \n",
    "    # Add key metrics to CSV\n",
    "    key_metrics = [\n",
    "        ('Accuracy', detailed_metrics['accuracy']),\n",
    "        ('Precision (Macro)', detailed_metrics['precision_macro']),\n",
    "        ('Recall (Macro)', detailed_metrics['recall_macro']),\n",
    "        ('F1-Score (Macro)', detailed_metrics['f1_macro']),\n",
    "        ('Precision (Weighted)', detailed_metrics['precision_weighted']),\n",
    "        ('Recall (Weighted)', detailed_metrics['recall_weighted']),\n",
    "        ('F1-Score (Weighted)', detailed_metrics['f1_weighted']),\n",
    "        ('ROC-AUC', detailed_metrics['roc_auc']),\n",
    "        ('PR-AUC', detailed_metrics['pr_auc']),\n",
    "        ('Specificity', detailed_metrics['specificity']),\n",
    "        ('Sensitivity', detailed_metrics['sensitivity']),\n",
    "        ('Test Loss', test_loss),\n",
    "        ('Target Achieved (95%)', 'Yes' if detailed_metrics['accuracy'] >= 0.95 else 'No')\n",
    "    ]\n",
    "    \n",
    "    for metric_name, value in key_metrics:\n",
    "        csv_data['Metric'].append(metric_name)\n",
    "        csv_data['Value'].append(value)\n",
    "    \n",
    "    # Add per-class metrics\n",
    "    for class_name, metrics in detailed_metrics['per_class_metrics'].items():\n",
    "        for metric_type, value in metrics.items():\n",
    "            if metric_type != 'support':\n",
    "                csv_data['Metric'].append(f\"{class_name}_{metric_type}\")\n",
    "                csv_data['Value'].append(value)\n",
    "    \n",
    "    csv_df = pd.DataFrame(csv_data)\n",
    "    csv_path = reports_dir / f\"metrics_summary_{CONFIG['model_name']}.csv\"\n",
    "    csv_df.to_csv(csv_path, index=False)\n",
    "    print(f\"✓ CSV metrics saved to: {csv_path}\")\n",
    "    \n",
    "    # Save detailed error analysis CSV\n",
    "    error_df_path = reports_dir / f\"error_analysis_{CONFIG['model_name']}.csv\"\n",
    "    error_analysis['error_df'].to_csv(error_df_path, index=False)\n",
    "    print(f\"✓ Error analysis CSV saved to: {error_df_path}\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(f\"\\n\" + \"=\" * 70)\n",
    "    print(\"FINAL EVALUATION SUMMARY\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(f\"\\n🎯 TARGET ACHIEVEMENT:\")\n",
    "    target_achieved = detailed_metrics['accuracy'] >= 0.95\n",
    "    print(f\"  95% Accuracy Target: {'✅ ACHIEVED' if target_achieved else '❌ NOT ACHIEVED'}\")\n",
    "    print(f\"  Actual Test Accuracy: {detailed_metrics['accuracy']:.1%}\")\n",
    "    \n",
    "    print(f\"\\n📊 KEY METRICS:\")\n",
    "    print(f\"  Accuracy: {detailed_metrics['accuracy']:.4f}\")\n",
    "    print(f\"  F1-Score (Macro): {detailed_metrics['f1_macro']:.4f}\")\n",
    "    print(f\"  ROC-AUC: {detailed_metrics['roc_auc']:.4f}\")\n",
    "    print(f\"  Precision (Macro): {detailed_metrics['precision_macro']:.4f}\")\n",
    "    print(f\"  Recall (Macro): {detailed_metrics['recall_macro']:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📁 GENERATED ARTIFACTS:\")\n",
    "    print(f\"  📊 Confusion Matrix: reports/confusion_matrix_detailed_{CONFIG['model_name']}.png\")\n",
    "    print(f\"  📈 ROC & PR Curves: reports/roc_pr_curves_{CONFIG['model_name']}.png\")\n",
    "    print(f\"  📋 JSON Report: reports/final_report_{CONFIG['model_name']}.json\")\n",
    "    print(f\"  📄 CSV Metrics: reports/metrics_summary_{CONFIG['model_name']}.csv\")\n",
    "    print(f\"  🔍 Error Analysis: reports/error_analysis_{CONFIG['model_name']}.csv\")\n",
    "    print(f\"  💾 Best Model: models/best_{CONFIG['model_name']}_binary.pth\")\n",
    "    \n",
    "    return final_report\n",
    "\n",
    "# Generate documentation and final report\n",
    "if 'detailed_metrics' in locals():\n",
    "    print(\"Generating comprehensive documentation and final report...\")\n",
    "    \n",
    "    # Document dataset splits\n",
    "    dataset_doc = document_dataset_splits()\n",
    "    \n",
    "    # Generate final comprehensive report\n",
    "    final_report = generate_final_report(\n",
    "        detailed_metrics, \n",
    "        curve_data, \n",
    "        dataset_doc, \n",
    "        error_analysis, \n",
    "        best_epoch\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n✅ Comprehensive evaluation and reporting completed!\")\n",
    "    print(f\"📂 All artifacts saved to: {project_root / 'reports'}\")\n",
    "    \n",
    "else:\n",
    "    print(\"⚠️  Please run the evaluation cells first to generate the final report\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_tumor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8dd8706d",
   "metadata": {},
   "source": [
    "# MRI vs Breast Histopathology Classification - Part 1: Data Preparation\n",
    "\n",
    "**Objective**: Build a binary classifier to distinguish between MRI brain scans and breast histopathology images with >95% accuracy.\n",
    "\n",
    "## Part 1: Dataset Organization and Preprocessing\n",
    "- Organize MRI data from 4 tumor classes into single MRI folder\n",
    "- Extract and organize breast histopathology data into BreastHisto folder\n",
    "- Create train/validation/test splits\n",
    "- Implement data preprocessing pipeline\n",
    "\n",
    "**Dataset Overview**:\n",
    "- **MRI**: Brain tumor images from 4 classes (glioma, meningioma, no_tumor, pituitary)\n",
    "- **BreastHisto**: Breast histopathology patches (IDC+ and IDC-)\n",
    "- **Target**: Binary classification between the two modalities\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a144840f",
   "metadata": {},
   "source": [
    "## 1. Environment Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3093341",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import os\n",
    "import sys\n",
    "import shutil\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Image processing\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "print(\"‚úÖ Libraries imported successfully\")\n",
    "print(f\"Random seed set to: {SEED}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88b06c7",
   "metadata": {},
   "source": [
    "## 2. Project Structure and Path Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780a2a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project paths\n",
    "project_root = Path.cwd().parent\n",
    "data_dir = project_root / 'data'\n",
    "raw_data_dir = data_dir / 'raw'\n",
    "processed_data_dir = data_dir / 'processed'\n",
    "\n",
    "# Source data paths\n",
    "tumor_source = project_root / 'tumor' / 'tumor'  # MRI data\n",
    "breasthisto_source = project_root / 'archive_2'  # Breast histopathology data\n",
    "\n",
    "# Create directories\n",
    "raw_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"üìÅ Project Structure:\")\n",
    "print(f\"  Project root: {project_root}\")\n",
    "print(f\"  Data directory: {data_dir}\")\n",
    "print(f\"  Raw data: {raw_data_dir}\")\n",
    "print(f\"  Processed data: {processed_data_dir}\")\n",
    "print(f\"\\nüìÇ Source Data:\")\n",
    "print(f\"  MRI source: {tumor_source}\")\n",
    "print(f\"  BreastHisto source: {breasthisto_source}\")\n",
    "\n",
    "# Verify source directories exist\n",
    "print(f\"\\nüîç Source Directory Verification:\")\n",
    "print(f\"  MRI source exists: {tumor_source.exists()}\")\n",
    "print(f\"  BreastHisto source exists: {breasthisto_source.exists()}\")\n",
    "\n",
    "if tumor_source.exists():\n",
    "    mri_classes = list(tumor_source.glob('*'))\n",
    "    print(f\"  MRI classes found: {[cls.name for cls in mri_classes if cls.is_dir()]}\")\n",
    "    \n",
    "if breasthisto_source.exists():\n",
    "    breasthisto_folders = list(breasthisto_source.glob('*'))\n",
    "    print(f\"  BreastHisto folders: {len([f for f in breasthisto_folders if f.is_dir()])} patient folders\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337ac589",
   "metadata": {},
   "source": [
    "## 3. MRI Data Organization\n",
    "\n",
    "Consolidate all MRI images from 4 tumor classes into a single MRI folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59396f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_mri_data(source_dir, target_dir, max_samples_per_class=None, convert_to_grayscale=True):\n",
    "    \"\"\"\n",
    "    Organize MRI data from multiple tumor classes into single MRI folder with grayscale conversion\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Path to tumor source directory\n",
    "        target_dir: Path to target MRI directory\n",
    "        max_samples_per_class: Maximum samples per class (for balancing)\n",
    "        convert_to_grayscale: Whether to convert images to grayscale during processing\n",
    "    \"\"\"\n",
    "    print(\"üß† Organizing MRI Data with Grayscale Conversion...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create target directory\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    tumor_classes = ['glioma_tumor', 'meningioma_tumor', 'no_tumor', 'pituitary_tumor']\n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.tiff', '.tif', '.bmp']\n",
    "    \n",
    "    total_copied = 0\n",
    "    class_counts = {}\n",
    "    \n",
    "    for class_name in tumor_classes:\n",
    "        class_dir = source_dir / class_name\n",
    "        \n",
    "        if not class_dir.exists():\n",
    "            print(f\"‚ö†Ô∏è  {class_name} directory not found: {class_dir}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüìÇ Processing {class_name}...\")\n",
    "        \n",
    "        # Get all image files\n",
    "        image_files = []\n",
    "        for ext in image_extensions:\n",
    "            image_files.extend(list(class_dir.glob(f'*{ext}')))\n",
    "            image_files.extend(list(class_dir.glob(f'*{ext.upper()}')))\n",
    "        \n",
    "        print(f\"  Found {len(image_files)} images\")\n",
    "        \n",
    "        # Limit samples if specified\n",
    "        if max_samples_per_class and len(image_files) > max_samples_per_class:\n",
    "            random.shuffle(image_files)\n",
    "            image_files = image_files[:max_samples_per_class]\n",
    "            print(f\"  Limited to {max_samples_per_class} images\")\n",
    "        \n",
    "        # Process and save images\n",
    "        copied_count = 0\n",
    "        for i, img_path in enumerate(tqdm(image_files, desc=f\"Processing {class_name}\")):\n",
    "            try:\n",
    "                # Create new filename with class prefix (always save as .png for consistency)\n",
    "                new_filename = f\"mri_{class_name}_{i:04d}.png\"\n",
    "                target_path = target_dir / new_filename\n",
    "                \n",
    "                if convert_to_grayscale:\n",
    "                    # Load image and convert to grayscale\n",
    "                    img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "                    if img is None:\n",
    "                        # Fallback to PIL if cv2 fails\n",
    "                        img = np.array(Image.open(img_path).convert('L'))\n",
    "                    \n",
    "                    # Save as grayscale PNG\n",
    "                    cv2.imwrite(str(target_path), img)\n",
    "                else:\n",
    "                    # Copy file as-is\n",
    "                    shutil.copy2(img_path, target_path)\n",
    "                \n",
    "                copied_count += 1\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"    ‚ö†Ô∏è  Error processing {img_path.name}: {e}\")\n",
    "        \n",
    "        class_counts[class_name] = copied_count\n",
    "        total_copied += copied_count\n",
    "        print(f\"  ‚úÖ Processed {copied_count} images to grayscale\")\n",
    "    \n",
    "    print(f\"\\nüìä MRI Data Organization Summary:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        percentage = (count / total_copied * 100) if total_copied > 0 else 0\n",
    "        print(f\"  {class_name}: {count:,} images ({percentage:.1f}%)\")\n",
    "    print(f\"  Total MRI images: {total_copied:,}\")\n",
    "    if convert_to_grayscale:\n",
    "        print(f\"  ‚úÖ All images converted to grayscale PNG format\")\n",
    "    \n",
    "    return class_counts, total_copied\n",
    "\n",
    "# Organize MRI data with grayscale conversion\n",
    "mri_target_dir = raw_data_dir / 'MRI'\n",
    "\n",
    "if tumor_source.exists():\n",
    "    mri_class_counts, mri_total = organize_mri_data(\n",
    "        tumor_source, \n",
    "        mri_target_dir, \n",
    "        max_samples_per_class=2000,  # Increased for better dataset size\n",
    "        convert_to_grayscale=True    # Enable grayscale conversion\n",
    "    )\n",
    "else:\n",
    "    print(f\"‚ùå MRI source directory not found: {tumor_source}\")\n",
    "    mri_class_counts, mri_total = {}, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b32805",
   "metadata": {},
   "source": [
    "## 4. Breast Histopathology Data Organization\n",
    "\n",
    "Extract and organize breast histopathology images from the archive_2 structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b64ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def organize_breasthisto_data(source_dir, target_dir, max_samples=None, convert_to_grayscale=True):\n",
    "    \"\"\"\n",
    "    Organize breast histopathology data from archive_2 structure with grayscale conversion\n",
    "    \n",
    "    Args:\n",
    "        source_dir: Path to archive_2 directory\n",
    "        target_dir: Path to target BreastHisto directory  \n",
    "        max_samples: Maximum total samples to copy\n",
    "        convert_to_grayscale: Whether to convert images to grayscale during processing\n",
    "    \"\"\"\n",
    "    print(\"üî¨ Organizing Breast Histopathology Data with Grayscale Conversion...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Create target directory\n",
    "    target_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    image_extensions = ['.jpg', '.jpeg', '.png', '.tiff', '.tif', '.bmp']\n",
    "    total_copied = 0\n",
    "    class_counts = {'IDC_negative': 0, 'IDC_positive': 0}\n",
    "    \n",
    "    # Look for the main data directory\n",
    "    data_root = source_dir / 'IDC_regular_ps50_idx5'\n",
    "    if not data_root.exists():\n",
    "        # Alternative: look for patient folders directly in source_dir\n",
    "        data_root = source_dir\n",
    "        \n",
    "    print(f\"üìÇ Scanning directory: {data_root}\")\n",
    "    \n",
    "    # Get all patient folders\n",
    "    patient_folders = [f for f in data_root.iterdir() if f.is_dir() and f.name.isdigit()]\n",
    "    print(f\"Found {len(patient_folders)} patient folders\")\n",
    "    \n",
    "    if max_samples:\n",
    "        print(f\"Will collect maximum {max_samples} samples\")\n",
    "    \n",
    "    # Process each patient folder\n",
    "    for patient_folder in tqdm(patient_folders, desc=\"Processing patients\"):\n",
    "        if max_samples and total_copied >= max_samples:\n",
    "            break\n",
    "            \n",
    "        # Each patient has class 0 (IDC-) and class 1 (IDC+) folders\n",
    "        for class_folder in ['0', '1']:\n",
    "            class_path = patient_folder / class_folder\n",
    "            \n",
    "            if not class_path.exists():\n",
    "                continue\n",
    "                \n",
    "            # Get all images in this class folder\n",
    "            image_files = []\n",
    "            for ext in image_extensions:\n",
    "                image_files.extend(list(class_path.glob(f'*{ext}')))\n",
    "                image_files.extend(list(class_path.glob(f'*{ext.upper()}')))\n",
    "            \n",
    "            # Process images with appropriate naming\n",
    "            for img_path in image_files:\n",
    "                if max_samples and total_copied >= max_samples:\n",
    "                    break\n",
    "                    \n",
    "                try:\n",
    "                    # Determine class name\n",
    "                    class_name = 'IDC_negative' if class_folder == '0' else 'IDC_positive'\n",
    "                    \n",
    "                    # Create new filename (always save as .png for consistency)\n",
    "                    new_filename = f\"breasthisto_{class_name}_{patient_folder.name}_{img_path.stem}.png\"\n",
    "                    target_path = target_dir / new_filename\n",
    "                    \n",
    "                    if convert_to_grayscale:\n",
    "                        # Load image and convert to grayscale\n",
    "                        img = cv2.imread(str(img_path), cv2.IMREAD_GRAYSCALE)\n",
    "                        if img is None:\n",
    "                            # Fallback to PIL if cv2 fails\n",
    "                            img = np.array(Image.open(img_path).convert('L'))\n",
    "                        \n",
    "                        # Save as grayscale PNG\n",
    "                        cv2.imwrite(str(target_path), img)\n",
    "                    else:\n",
    "                        # Copy file as-is\n",
    "                        shutil.copy2(img_path, target_path)\n",
    "                    \n",
    "                    class_counts[class_name] += 1\n",
    "                    total_copied += 1\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ö†Ô∏è  Error processing {img_path.name}: {e}\")\n",
    "    \n",
    "    print(f\"\\nüìä Breast Histopathology Data Organization Summary:\")\n",
    "    for class_name, count in class_counts.items():\n",
    "        percentage = (count / total_copied * 100) if total_copied > 0 else 0\n",
    "        print(f\"  {class_name}: {count:,} images ({percentage:.1f}%)\")\n",
    "    print(f\"  Total BreastHisto images: {total_copied:,}\")\n",
    "    if convert_to_grayscale:\n",
    "        print(f\"  ‚úÖ All images converted to grayscale PNG format\")\n",
    "    \n",
    "    return class_counts, total_copied\n",
    "\n",
    "# Organize Breast Histopathology data with grayscale conversion\n",
    "breasthisto_target_dir = raw_data_dir / 'BreastHisto'\n",
    "\n",
    "if breasthisto_source.exists():\n",
    "    breasthisto_class_counts, breasthisto_total = organize_breasthisto_data(\n",
    "        breasthisto_source, \n",
    "        breasthisto_target_dir,\n",
    "        max_samples=8000,           # Increased to balance with MRI data  \n",
    "        convert_to_grayscale=True   # Enable grayscale conversion\n",
    "    )\n",
    "else:\n",
    "    print(f\"‚ùå BreastHisto source directory not found: {breasthisto_source}\")\n",
    "    breasthisto_class_counts, breasthisto_total = {}, 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45825102",
   "metadata": {},
   "source": [
    "## 5. Dataset Statistics and Verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b69635",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_organized_data(raw_data_dir):\n",
    "    \"\"\"\n",
    "    Analyze the organized raw data and display statistics\n",
    "    \"\"\"\n",
    "    print(\"üìä Dataset Analysis\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    classes = ['MRI', 'BreastHisto']\n",
    "    total_images = 0\n",
    "    class_stats = {}\n",
    "    \n",
    "    for class_name in classes:\n",
    "        class_dir = raw_data_dir / class_name\n",
    "        \n",
    "        if class_dir.exists():\n",
    "            # Count images\n",
    "            image_files = []\n",
    "            for ext in ['*.jpg', '*.jpeg', '*.png', '*.tiff', '*.tif', '*.bmp']:\n",
    "                image_files.extend(list(class_dir.glob(ext)))\n",
    "                image_files.extend(list(class_dir.glob(ext.upper())))\n",
    "            \n",
    "            count = len(image_files)\n",
    "            class_stats[class_name] = count\n",
    "            total_images += count\n",
    "            \n",
    "            print(f\"\\nüìÇ {class_name}:\")\n",
    "            print(f\"  Images: {count:,}\")\n",
    "            print(f\"  Directory: {class_dir}\")\n",
    "            \n",
    "            # Sample a few images to check\n",
    "            sample_images = random.sample(image_files, min(3, len(image_files)))\n",
    "            print(f\"  Sample files: {[img.name for img in sample_images]}\")\n",
    "            \n",
    "        else:\n",
    "            print(f\"\\n‚ùå {class_name}: Directory not found\")\n",
    "            class_stats[class_name] = 0\n",
    "    \n",
    "    print(f\"\\nüìà Overall Statistics:\")\n",
    "    print(f\"  Total images: {total_images:,}\")\n",
    "    \n",
    "    if total_images > 0:\n",
    "        for class_name, count in class_stats.items():\n",
    "            percentage = (count / total_images) * 100\n",
    "            print(f\"  {class_name}: {count:,} ({percentage:.1f}%)\")\n",
    "    \n",
    "    return class_stats, total_images\n",
    "\n",
    "# Analyze organized data\n",
    "final_class_stats, final_total = analyze_organized_data(raw_data_dir)\n",
    "\n",
    "# Check data balance\n",
    "print(f\"\\n‚öñÔ∏è  Data Balance Analysis:\")\n",
    "if final_total > 0:\n",
    "    mri_count = final_class_stats.get('MRI', 0)\n",
    "    breasthisto_count = final_class_stats.get('BreastHisto', 0)\n",
    "    \n",
    "    if mri_count > 0 and breasthisto_count > 0:\n",
    "        ratio = max(mri_count, breasthisto_count) / min(mri_count, breasthisto_count)\n",
    "        print(f\"  Class ratio: {ratio:.2f}:1\")\n",
    "        \n",
    "        if ratio <= 2.0:\n",
    "            print(f\"  ‚úÖ Classes are reasonably balanced\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Classes are imbalanced - consider balancing strategies\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå One or both classes are missing\")\n",
    "else:\n",
    "    print(f\"  ‚ùå No data found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343fad0e",
   "metadata": {},
   "source": [
    "## 6. Data Splitting (Train/Validation/Test)\n",
    "\n",
    "Create balanced train/validation/test splits for both classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f786be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "def create_data_splits(raw_data_dir, processed_data_dir, split_ratios=(0.7, 0.15, 0.15), random_seed=42):\n",
    "    \"\"\"\n",
    "    Create train/validation/test splits from organized raw data, ensuring NO overlap.\n",
    "    \n",
    "    Args:\n",
    "        raw_data_dir: Path to raw organized data\n",
    "        processed_data_dir: Path to processed data directory\n",
    "        split_ratios: Tuple of (train, val, test) ratios\n",
    "        random_seed: Random seed for reproducible splits\n",
    "    \"\"\"\n",
    "    print(\"‚úÇÔ∏è  Creating Data Splits\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Set random seed for reproducible splits\n",
    "    random.seed(random_seed)\n",
    "    print(f\"üé≤ Using random seed: {random_seed}\")\n",
    "    \n",
    "    # Clean up the destination directory to ensure a fresh start\n",
    "    if processed_data_dir.exists():\n",
    "        print(f\"üßπ Cleaning existing processed data directory: {processed_data_dir}\")\n",
    "        shutil.rmtree(processed_data_dir)\n",
    "    \n",
    "    # Re-create the base directory after cleaning\n",
    "    print(f\"‚ú® Creating new processed data directory: {processed_data_dir}\")\n",
    "    processed_data_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_ratio, val_ratio, test_ratio = split_ratios\n",
    "    print(f\"Split ratios - Train: {train_ratio:.1%}, Val: {val_ratio:.1%}, Test: {test_ratio:.1%}\")\n",
    "    \n",
    "    # Create split subdirectories\n",
    "    splits = ['train', 'val', 'test']\n",
    "    classes = ['MRI', 'BreastHisto']\n",
    "    \n",
    "    for split in splits:\n",
    "        for class_name in classes:\n",
    "            split_dir = processed_data_dir / split / class_name\n",
    "            split_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    split_stats = {}\n",
    "    all_processed_files = set()  # Track all processed files globally to prevent duplicates\n",
    "    \n",
    "    # Process each class\n",
    "    for class_name in classes:\n",
    "        raw_class_dir = raw_data_dir / class_name\n",
    "        \n",
    "        if not raw_class_dir.exists():\n",
    "            print(f\"‚ö†Ô∏è  {class_name} directory not found: {raw_class_dir}\")\n",
    "            continue\n",
    "            \n",
    "        print(f\"\\nüìÇ Splitting {class_name} data...\")\n",
    "        \n",
    "        # Get all image files with deduplication\n",
    "        image_files = set()  # Use set to automatically handle duplicates\n",
    "        extensions = ['jpg', 'jpeg', 'png', 'tiff', 'tif', 'bmp']\n",
    "        \n",
    "        for ext in extensions:\n",
    "            # Check both lowercase and uppercase extensions\n",
    "            for pattern in [f'*.{ext}', f'*.{ext.upper()}']:\n",
    "                for file_path in raw_class_dir.glob(pattern):\n",
    "                    # Resolve the path to handle any symlinks/relative paths\n",
    "                    resolved_path = file_path.resolve()\n",
    "                    image_files.add(resolved_path)\n",
    "        \n",
    "        # Convert back to list for indexing\n",
    "        image_files = list(image_files)\n",
    "        print(f\"  Total unique images found: {len(image_files)}\")\n",
    "        \n",
    "        # Check for global duplicates\n",
    "        current_file_names = {f.name for f in image_files}\n",
    "        global_overlaps = current_file_names.intersection(all_processed_files)\n",
    "        if global_overlaps:\n",
    "            print(f\"  ‚ö†Ô∏è  Warning: {len(global_overlaps)} files already processed in another class\")\n",
    "            # Remove duplicates to prevent cross-class contamination\n",
    "            image_files = [f for f in image_files if f.name not in all_processed_files]\n",
    "            print(f\"  After deduplication: {len(image_files)} images\")\n",
    "        \n",
    "        # Add current files to global tracker\n",
    "        all_processed_files.update(f.name for f in image_files)\n",
    "        \n",
    "        if len(image_files) == 0:\n",
    "            print(f\"  ‚ö†Ô∏è  No unique images found for {class_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Shuffle for random splits (using the set seed)\n",
    "        random.shuffle(image_files)\n",
    "        \n",
    "        # Calculate split indices\n",
    "        n_total = len(image_files)\n",
    "        n_train = int(n_total * train_ratio)\n",
    "        n_val = int(n_total * val_ratio)\n",
    "        \n",
    "        # Create non-overlapping splits using slicing\n",
    "        train_files = image_files[:n_train]\n",
    "        val_files = image_files[n_train:n_train + n_val]\n",
    "        test_files = image_files[n_train + n_val:]\n",
    "        \n",
    "        print(f\"  Split sizes - Train: {len(train_files)}, Val: {len(val_files)}, Test: {len(test_files)}\")\n",
    "        \n",
    "        # Verify no overlaps within this class\n",
    "        train_set = set(f.name for f in train_files)\n",
    "        val_set = set(f.name for f in val_files)\n",
    "        test_set = set(f.name for f in test_files)\n",
    "        \n",
    "        train_val_overlap = train_set.intersection(val_set)\n",
    "        train_test_overlap = train_set.intersection(test_set)\n",
    "        val_test_overlap = val_set.intersection(test_set)\n",
    "        \n",
    "        if train_val_overlap or train_test_overlap or val_test_overlap:\n",
    "            print(f\"  ‚ùå ERROR: Overlap detected within {class_name}!\")\n",
    "            print(f\"    Train-Val: {len(train_val_overlap)}, Train-Test: {len(train_test_overlap)}, Val-Test: {len(val_test_overlap)}\")\n",
    "            continue\n",
    "        else:\n",
    "            print(f\"  ‚úÖ No overlaps detected within {class_name}\")\n",
    "        \n",
    "        # Copy files to respective directories\n",
    "        split_data = {\n",
    "            'train': train_files,\n",
    "            'val': val_files,\n",
    "            'test': test_files\n",
    "        }\n",
    "        \n",
    "        class_split_stats = {}\n",
    "        \n",
    "        for split, files in split_data.items():\n",
    "            split_dir = processed_data_dir / split / class_name\n",
    "            copied_count = 0\n",
    "            \n",
    "            print(f\"  üìÅ Copying {len(files)} files to {split}...\")\n",
    "            for img_file in tqdm(files, desc=f\"    {split}\", leave=False):\n",
    "                try:\n",
    "                    target_path = split_dir / img_file.name\n",
    "                    \n",
    "                    # Double-check that target doesn't already exist\n",
    "                    if target_path.exists():\n",
    "                        print(f\"    ‚ö†Ô∏è  Target already exists: {target_path.name}\")\n",
    "                        continue\n",
    "                    \n",
    "                    shutil.copy2(img_file, target_path)\n",
    "                    copied_count += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"    ‚ùå Error copying {img_file.name}: {e}\")\n",
    "            \n",
    "            class_split_stats[split] = copied_count\n",
    "            print(f\"  ‚úÖ Successfully copied {copied_count} files to {split}/{class_name}\")\n",
    "        \n",
    "        split_stats[class_name] = class_split_stats\n",
    "    \n",
    "    # Display final split statistics\n",
    "    print(f\"\\nüìä Final Data Split Summary:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    global_train_files = set()\n",
    "    global_val_files = set()\n",
    "    global_test_files = set()\n",
    "    \n",
    "    for split in splits:\n",
    "        print(f\"\\n{split.upper()} SET:\")\n",
    "        split_total = 0\n",
    "        \n",
    "        # Collect files for global overlap check\n",
    "        split_files = set()\n",
    "        for class_name in classes:\n",
    "            split_dir = processed_data_dir / split / class_name\n",
    "            if split_dir.exists():\n",
    "                class_files = {f.name for f in split_dir.iterdir() if f.is_file()}\n",
    "                split_files.update(class_files)\n",
    "        \n",
    "        # Store for global check\n",
    "        if split == 'train':\n",
    "            global_train_files = split_files\n",
    "        elif split == 'val':\n",
    "            global_val_files = split_files\n",
    "        elif split == 'test':\n",
    "            global_test_files = split_files\n",
    "        \n",
    "        for class_name in classes:\n",
    "            count = split_stats.get(class_name, {}).get(split, 0)\n",
    "            split_total += count\n",
    "            print(f\"  {class_name}: {count:,} images\")\n",
    "        \n",
    "        print(f\"  Total: {split_total:,} images\")\n",
    "        \n",
    "        # Calculate class balance within split\n",
    "        if split_total > 0:\n",
    "            print(\"  Class distribution:\")\n",
    "            for class_name in classes:\n",
    "                count = split_stats.get(class_name, {}).get(split, 0)\n",
    "                percentage = (count / split_total) * 100\n",
    "                print(f\"    {class_name}: {percentage:.1f}%\")\n",
    "    \n",
    "    # Final global overlap verification\n",
    "    print(f\"\\nüîç Final Overlap Verification:\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    train_val_final = global_train_files.intersection(global_val_files)\n",
    "    train_test_final = global_train_files.intersection(global_test_files)\n",
    "    val_test_final = global_val_files.intersection(global_test_files)\n",
    "    \n",
    "    print(f\"Train-Val overlap: {len(train_val_final)} files\")\n",
    "    print(f\"Train-Test overlap: {len(train_test_final)} files\")\n",
    "    print(f\"Val-Test overlap: {len(val_test_final)} files\")\n",
    "    \n",
    "    if train_val_final or train_test_final or val_test_final:\n",
    "        print(\"‚ùå OVERLAP STILL DETECTED!\")\n",
    "        if train_val_final:\n",
    "            print(f\"  Train-Val overlaps: {list(train_val_final)[:5]}...\")\n",
    "        if train_test_final:\n",
    "            print(f\"  Train-Test overlaps: {list(train_test_final)[:5]}...\")\n",
    "        if val_test_final:\n",
    "            print(f\"  Val-Test overlaps: {list(val_test_final)[:5]}...\")\n",
    "    else:\n",
    "        print(\"‚úÖ SUCCESS: No overlaps detected between splits!\")\n",
    "    \n",
    "    return split_stats\n",
    "\n",
    "\n",
    "# Usage with your existing code:\n",
    "if final_total > 0:\n",
    "    split_statistics = create_data_splits(\n",
    "        raw_data_dir, \n",
    "        processed_data_dir,\n",
    "        split_ratios=(0.7, 0.15, 0.15),\n",
    "        random_seed=42  # For reproducible results\n",
    "    )\n",
    "else:\n",
    "    print(\"‚ùå No data available for splitting\")\n",
    "    split_statistics = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-verify-markdown",
   "metadata": {},
   "source": [
    "### 6.1. Data Split Verification\n",
    "\n",
    "To ensure model integrity, we must verify that there is absolutely no overlap (data leakage) between the training, validation, and test sets. We do this by collecting all filenames from each split and checking for common elements between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-verify-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_data_splits_no_overlap(processed_data_dir):\n",
    "    \"\"\"\n",
    "    Verifies that there is no file name overlap between train, val, and test sets.\n",
    "    \"\"\"\n",
    "    print(\"üõ°Ô∏è Verifying Data Splits for Overlap...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    splits = ['train', 'val', 'test']\n",
    "    classes = ['MRI', 'BreastHisto']\n",
    "    \n",
    "    # Collect all file names for each split\n",
    "    file_sets = {split: set() for split in splits}\n",
    "    \n",
    "    all_dirs_found = True\n",
    "    for split in splits:\n",
    "        for class_name in classes:\n",
    "            split_dir = processed_data_dir / split / class_name\n",
    "            if not split_dir.exists():\n",
    "                print(f\"‚ö†Ô∏è Directory not found: {split_dir}\")\n",
    "                all_dirs_found = False\n",
    "                continue\n",
    "            \n",
    "            files = {f.name for f in split_dir.iterdir()}\n",
    "            file_sets[split].update(files)\n",
    "    \n",
    "    if not all_dirs_found:\n",
    "        print(\"‚ùå Verification aborted due to missing directories.\")\n",
    "        return\n",
    "        \n",
    "    # Check for overlaps\n",
    "    train_val_overlap = file_sets['train'].intersection(file_sets['val'])\n",
    "    train_test_overlap = file_sets['train'].intersection(file_sets['test'])\n",
    "    val_test_overlap = file_sets['val'].intersection(file_sets['test'])\n",
    "    \n",
    "    print(f\"  Train set size: {len(file_sets['train']):,}\")\n",
    "    print(f\"  Validation set size: {len(file_sets['val']):,}\")\n",
    "    print(f\"  Test set size: {len(file_sets['test']):,}\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"  Train-Validation Overlap: {len(train_val_overlap)} files\")\n",
    "    print(f\"  Train-Test Overlap: {len(train_test_overlap)} files\")\n",
    "    print(f\"  Validation-Test Overlap: {len(val_test_overlap)} files\")\n",
    "    \n",
    "    if not train_val_overlap and not train_test_overlap and not val_test_overlap:\n",
    "        print(\"\\n‚úÖ SUCCESS: No overlap found between data splits. Data is properly separated.\")\n",
    "    else:\n",
    "        print(\"\\n‚ùå FAILURE: Overlap detected between data splits! This will cause data leakage.\")\n",
    "        if train_val_overlap:\n",
    "            print(f\"   - Overlapping files (train/val): {list(train_val_overlap)[:5]}...\")\n",
    "        if train_test_overlap:\n",
    "            print(f\"   - Overlapping files (train/test): {list(train_test_overlap)[:5]}...\")\n",
    "        if val_test_overlap:\n",
    "            print(f\"   - Overlapping files (val/test): {list(val_test_overlap)[:5]}...\")\n",
    "\n",
    "# Run the verification\n",
    "if split_statistics:\n",
    "    verify_data_splits_no_overlap(processed_data_dir)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Cannot verify splits as they were not created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6631be91",
   "metadata": {},
   "source": [
    "## 7. Sample Visualization and Quality Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec90ad1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_split_samples(processed_data_dir, split_name, n_samples=4):\n",
    "    \"\"\"\n",
    "    Visualize sample images from a specific split (train, val, or test).\n",
    "    \"\"\"\n",
    "    print(f\"\\nüñºÔ∏è  Sample Image Visualization for '{split_name.upper()}' Set\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    classes = ['MRI', 'BreastHisto']\n",
    "    \n",
    "    fig, axes = plt.subplots(len(classes), n_samples, figsize=(15, 8))\n",
    "    fig.suptitle(f'Sample Images from {split_name.upper()} Set', fontsize=16)\n",
    "    \n",
    "    for class_idx, class_name in enumerate(classes):\n",
    "        data_dir = processed_data_dir / split_name / class_name\n",
    "        \n",
    "        if data_dir.exists():\n",
    "            image_files = list(data_dir.glob('*.png')) + list(data_dir.glob('*.jpg'))\n",
    "            \n",
    "            if len(image_files) >= n_samples:\n",
    "                sample_images = random.sample(image_files, n_samples)\n",
    "                for img_idx, img_path in enumerate(sample_images):\n",
    "                    try:\n",
    "                        img = Image.open(img_path)\n",
    "                        if img.mode != 'RGB':\n",
    "                            img = img.convert('RGB')\n",
    "                        \n",
    "                        ax = axes[class_idx, img_idx]\n",
    "                        ax.imshow(img)\n",
    "                        ax.axis('off')\n",
    "                        title = f\"{class_name}\\n{img.size[0]}x{img.size[1]}\"\n",
    "                        ax.set_title(title, fontsize=10)\n",
    "                    except Exception as e:\n",
    "                        ax = axes[class_idx, img_idx]\n",
    "                        ax.text(0.5, 0.5, 'Error', ha='center', va='center', transform=ax.transAxes)\n",
    "                        ax.set_title(f\"{class_name} (Error)\")\n",
    "                        print(f\"‚ö†Ô∏è Error loading {img_path}: {e}\")\n",
    "            else:\n",
    "                for img_idx in range(n_samples):\n",
    "                    ax = axes[class_idx, img_idx]\n",
    "                    ax.text(0.5, 0.5, 'Not enough images', ha='center', va='center', transform=ax.transAxes)\n",
    "                    ax.set_title(f\"{class_name} (Insufficient)\")\n",
    "        else:\n",
    "            for img_idx in range(n_samples):\n",
    "                ax = axes[class_idx, img_idx]\n",
    "                ax.text(0.5, 0.5, 'Directory not found', ha='center', va='center', transform=ax.transAxes)\n",
    "                ax.set_title(f\"{class_name} (Not Found)\")\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples for all splits if data exists\n",
    "if split_statistics:\n",
    "    for split in ['train', 'val', 'test']:\n",
    "        visualize_split_samples(processed_data_dir, split, n_samples=4)\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No split data available for visualization\")\n",
    "\n",
    "# Print image statistics from the training set\n",
    "print(f\"\\nüìè Image Statistics Analysis (from Train Set):\")\n",
    "if split_statistics:\n",
    "    for class_name in ['MRI', 'BreastHisto']:\n",
    "        train_dir = processed_data_dir / 'train' / class_name\n",
    "        if train_dir.exists():\n",
    "            print(f\"\\n{class_name}:\")\n",
    "            image_files = list(train_dir.glob('*.png')) + list(train_dir.glob('*.jpg'))\n",
    "            sample_size = min(50, len(image_files))\n",
    "            if sample_size > 0:\n",
    "                sample_images = random.sample(image_files, sample_size)\n",
    "                sizes, formats, modes = [], [], []\n",
    "                for img_path in sample_images:\n",
    "                    try:\n",
    "                        with Image.open(img_path) as img:\n",
    "                            sizes.append(img.size)\n",
    "                            formats.append(img.format)\n",
    "                            modes.append(img.mode)\n",
    "                    except Exception: continue\n",
    "                if sizes:\n",
    "                    widths, heights = [s[0] for s in sizes], [s[1] for s in sizes]\n",
    "                    print(f\"  Sample size: {len(sizes)} images\")\n",
    "                    print(f\"  Width range: {min(widths)} - {max(widths)} px\")\n",
    "                    print(f\"  Height range: {min(heights)} - {max(heights)} px\")\n",
    "                    print(f\"  Common formats: {Counter(formats).most_common(2)}\")\n",
    "                    print(f\"  Common modes: {Counter(modes).most_common(2)}\")\n",
    "            else:\n",
    "                print(\"  No images found for analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e14ded",
   "metadata": {},
   "source": [
    "## 8. Data Preparation Summary and Next Steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a87d2c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data_preparation_report(split_statistics, final_class_stats):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive report of data preparation results\n",
    "    \"\"\"\n",
    "    print(\"üìã Data Preparation Summary Report\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Overall statistics\n",
    "    total_images = sum(final_class_stats.values())\n",
    "    \n",
    "    print(f\"\\nüìä OVERALL DATA STATISTICS:\")\n",
    "    print(f\"  Total images collected: {total_images:,}\")\n",
    "    \n",
    "    for class_name, count in final_class_stats.items():\n",
    "        percentage = (count / total_images * 100) if total_images > 0 else 0\n",
    "        print(f\"  {class_name}: {count:,} images ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Split statistics\n",
    "    if split_statistics:\n",
    "        print(f\"\\nüìà DATA SPLIT BREAKDOWN:\")\n",
    "        \n",
    "        splits = ['train', 'val', 'test']\n",
    "        for split in splits:\n",
    "            split_total = 0\n",
    "            print(f\"\\n  {split.upper()} SET:\")\n",
    "            \n",
    "            for class_name in ['MRI', 'BreastHisto']:\n",
    "                count = split_statistics.get(class_name, {}).get(split, 0)\n",
    "                split_total += count\n",
    "                print(f\"    {class_name}: {count:,} images\")\n",
    "            \n",
    "            print(f\"    Total: {split_total:,} images\")\n",
    "            \n",
    "            # Class balance\n",
    "            if split_total > 0:\n",
    "                for class_name in ['MRI', 'BreastHisto']:\n",
    "                    count = split_statistics.get(class_name, {}).get(split, 0)\n",
    "                    percentage = (count / split_total) * 100\n",
    "                    print(f\"      {class_name}: {percentage:.1f}%\")\n",
    "    \n",
    "    # Data quality assessment\n",
    "    print(f\"\\n‚úÖ DATA QUALITY ASSESSMENT:\")\n",
    "    \n",
    "    # Check if we have data\n",
    "    if total_images > 0:\n",
    "        print(f\"  ‚úÖ Data collection: SUCCESS\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Data collection: FAILED\")\n",
    "        return\n",
    "    \n",
    "    # Check class balance\n",
    "    mri_count = final_class_stats.get('MRI', 0)\n",
    "    breasthisto_count = final_class_stats.get('BreastHisto', 0)\n",
    "    \n",
    "    if mri_count > 0 and breasthisto_count > 0:\n",
    "        ratio = max(mri_count, breasthisto_count) / min(mri_count, breasthisto_count)\n",
    "        if ratio <= 2.0:\n",
    "            print(f\"  ‚úÖ Class balance: GOOD (ratio {ratio:.2f}:1)\")\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  Class balance: IMBALANCED (ratio {ratio:.2f}:1)\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Class balance: MISSING CLASSES\")\n",
    "    \n",
    "    # Check minimum data requirements\n",
    "    min_required = 1000  # Minimum images per class for good training\n",
    "    \n",
    "    data_sufficient = all(count >= min_required for count in final_class_stats.values())\n",
    "    if data_sufficient:\n",
    "        print(f\"  ‚úÖ Data sufficiency: ADEQUATE (>{min_required} per class)\")\n",
    "    else:\n",
    "        print(f\"  ‚ö†Ô∏è  Data sufficiency: LIMITED (<{min_required} per class)\")\n",
    "    \n",
    "    # Directory structure\n",
    "    expected_dirs = [\n",
    "        processed_data_dir / 'train' / 'MRI',\n",
    "        processed_data_dir / 'train' / 'BreastHisto',\n",
    "        processed_data_dir / 'val' / 'MRI',\n",
    "        processed_data_dir / 'val' / 'BreastHisto',\n",
    "        processed_data_dir / 'test' / 'MRI',\n",
    "        processed_data_dir / 'test' / 'BreastHisto'\n",
    "    ]\n",
    "    \n",
    "    all_dirs_exist = all(dir_path.exists() for dir_path in expected_dirs)\n",
    "    if all_dirs_exist:\n",
    "        print(f\"  ‚úÖ Directory structure: COMPLETE\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Directory structure: INCOMPLETE\")\n",
    "    \n",
    "    # Recommendations\n",
    "    print(f\"\\nüí° RECOMMENDATIONS FOR NEXT STEPS:\")\n",
    "    \n",
    "    if data_sufficient and all_dirs_exist:\n",
    "        print(f\"  üöÄ Ready to proceed to Part 2: Model Architecture and Training\")\n",
    "        print(f\"  üìù Consider implementing data augmentation to increase effective dataset size\")\n",
    "        print(f\"  üéØ Target: Build custom CNN achieving >95% accuracy\")\n",
    "    else:\n",
    "        if not data_sufficient:\n",
    "            print(f\"  üìà Consider collecting more data or reducing train/val/test requirements\")\n",
    "        if not all_dirs_exist:\n",
    "            print(f\"  üîß Fix directory structure before proceeding\")\n",
    "    \n",
    "    # File paths for next parts\n",
    "    print(f\"\\nüìÅ KEY PATHS FOR NEXT PARTS:\")\n",
    "    print(f\"  Processed data: {processed_data_dir}\")\n",
    "    print(f\"  Train data: {processed_data_dir / 'train'}\")\n",
    "    print(f\"  Validation data: {processed_data_dir / 'val'}\")\n",
    "    print(f\"  Test data: {processed_data_dir / 'test'}\")\n",
    "    \n",
    "    return {\n",
    "        'total_images': total_images,\n",
    "        'class_stats': final_class_stats,\n",
    "        'split_stats': split_statistics,\n",
    "        'data_sufficient': data_sufficient,\n",
    "        'dirs_complete': all_dirs_exist,\n",
    "        'ready_for_training': data_sufficient and all_dirs_exist\n",
    "    }\n",
    "\n",
    "# Generate final report\n",
    "preparation_report = generate_data_preparation_report(split_statistics, final_class_stats)\n",
    "\n",
    "# Save preparation metadata for next parts\n",
    "import json\n",
    "\n",
    "metadata = {\n",
    "    'preparation_date': pd.Timestamp.now().isoformat(),\n",
    "    'total_images': final_total,\n",
    "    'class_statistics': final_class_stats,\n",
    "    'split_statistics': split_statistics,\n",
    "    'processed_data_path': str(processed_data_dir),\n",
    "    'ready_for_training': preparation_report.get('ready_for_training', False)\n",
    "}\n",
    "\n",
    "metadata_path = processed_data_dir / 'preparation_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\nüíæ Preparation metadata saved to: {metadata_path}\")\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"PART 1 COMPLETED: DATA PREPARATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚úÖ Data organization and splitting completed successfully!\")\n",
    "print(f\"üöÄ Ready to proceed to Part 2: Model Architecture and Training\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-cnn-markdown-title",
   "metadata": {},
   "source": [
    "## 9. Part 2 Preview: Custom Shallow CNN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-cnn-code-imports",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "# Imports for the visualization part\n",
    "import random\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "print(f\"PyTorch Version: {torch.__version__}\")\n",
    "\n",
    "# Set device (use GPU if available, otherwise CPU)\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-cnn-markdown-details",
   "metadata": {},
   "source": [
    "### Model Design\n",
    "\n",
    "For this binary classification task, a complex, deep architecture like ResNet or VGG is likely unnecessary and could lead to overfitting given the distinct nature of the two image classes. A custom, shallow CNN is a more efficient and interpretable starting point.\n",
    "\n",
    "- **Architecture**: A simple CNN with 3-4 convolutional blocks. Each block consists of a `Conv2D` layer to learn features, `BatchNormalization` to stabilize learning, and `MaxPooling2D` to downsample and create spatial invariance.\n",
    "- **Loss Function**: `binary_crossentropy` is the standard choice for a two-class classification problem.\n",
    "- **Evaluation**: We will monitor `accuracy` during training. For a comprehensive evaluation on the test set, we will use `precision`, `recall`, `F1-score`, and a `confusion matrix` to understand the model's performance on each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-cnn-code-model",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShallowCNN(nn.Module):\n",
    "    \"\"\"\n",
    "    A shallow Convolutional Neural Network model for binary classification,\n",
    "    equivalent to the provided Keras model.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_channels=1):\n",
    "        super(ShallowCNN, self).__init__()\n",
    "        \n",
    "        self.features = nn.Sequential(\n",
    "            # Convolutional Block 1\n",
    "            # Input: (N, 1, 128, 128)\n",
    "            nn.Conv2d(in_channels=input_channels, out_channels=32, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Output: (N, 32, 64, 64)\n",
    "            \n",
    "            # Convolutional Block 2\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Output: (N, 64, 32, 32)\n",
    "            \n",
    "            # Convolutional Block 3\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, padding='same'),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), # Output: (N, 128, 16, 16)\n",
    "        )\n",
    "        \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(), # Output: (N, 128 * 16 * 16 = 32768)\n",
    "            nn.Linear(128 * 16 * 16, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid() # Sigmoid for binary classification\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Defines the forward pass of the model.\"\"\"\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "new-breasthisto-markdown",
   "metadata": {},
   "source": [
    "### Focus: Breast Histopathology Images\n",
    "\n",
    "Breast histopathology images consist of small 50x50 pixel patches. The key distinction is between patches with Invasive Ductal Carcinoma (IDC+) and those without (IDC-).\n",
    "- **IDC- (Negative)**: Generally show more uniform, organized tissue structures with less cellular density.\n",
    "- **IDC+ (Positive)**: Often characterized by a higher density of cancer cells, irregular shapes, and darker staining nuclei."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "new-breasthisto-code",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the model\n",
    "# Assuming input images will be resized to 128x128 grayscale\n",
    "INPUT_SHAPE_PYTORCH = (1, 128, 128) # (Channels, Height, Width)\n",
    "model = ShallowCNN(input_channels=INPUT_SHAPE_PYTORCH[0]).to(device)\n",
    "\n",
    "# Print the model summary\n",
    "print(f\"\\nModel created for input shape: {INPUT_SHAPE_PYTORCH}\")\n",
    "summary(model, input_size=INPUT_SHAPE_PYTORCH)\n",
    "\n",
    "# Define Loss Function and Optimizer (equivalent to model.compile)\n",
    "LEARNING_RATE = 0.001\n",
    "criterion = nn.BCELoss() # Binary Cross-Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "print(\"\\n--- Optimizer and Loss Function ---\")\n",
    "print(f\"Optimizer: {optimizer.__class__.__name__}\")\n",
    "print(f\"Loss Function: {criterion.__class__.__name__}\")\n",
    "print(\"---------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2410ab6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Configuration ---\n",
      "Using device: cuda\n",
      "Training data path: c:\\Users\\Ammad\\Documents\\Projects\\Personal\\Brain\\data\\processed\\train\n",
      "Validation data path: c:\\Users\\Ammad\\Documents\\Projects\\Personal\\Brain\\data\\processed\\val\n",
      "Batch Size: 64, Epochs: 15, LR: 0.001\n",
      "---------------------\n",
      "\n",
      "‚úÖ Data loaded successfully!\n",
      "Found classes: ['BreastHisto', 'MRI'] -> {'BreastHisto': 0, 'MRI': 1}\n",
      "Training samples: 7857, Validation samples: 1683\n",
      "\n",
      "--- Epoch 1/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 123/123 [00:31<00:00,  3.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0321 | Train Acc: 0.9948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 27/27 [00:19<00:00,  1.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val Loss:   0.0000 | Val Acc:   1.0000\n",
      "\n",
      "--- Epoch 2/15 ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  41%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 51/123 [00:17<00:24,  2.89it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model, history\n\u001b[32m    143\u001b[39m \u001b[38;5;66;03m# 6. Start the Training Process\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m trained_model, training_history = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    145\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    146\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    149\u001b[39m \u001b[43m    \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    150\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mNUM_EPOCHS\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;66;03m# ===================================================================\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m#               END OF TRAINING SCRIPT (CORRECTED)\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;66;03m# ===================================================================\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 93\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m(model, criterion, optimizer, train_loader, val_loader, num_epochs)\u001b[39m\n\u001b[32m     90\u001b[39m running_loss = \u001b[32m0.0\u001b[39m\n\u001b[32m     91\u001b[39m running_corrects = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m93\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtqdm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdesc\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mTraining\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mview\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDEVICE\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ammad\\Documents\\Projects\\Personal\\Brain\\brain_tumor_env\\Lib\\site-packages\\tqdm\\std.py:1181\u001b[39m, in \u001b[36mtqdm.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1178\u001b[39m time = \u001b[38;5;28mself\u001b[39m._time\n\u001b[32m   1180\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1182\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01myield\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mobj\u001b[49m\n\u001b[32m   1183\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Update and possibly print the progressbar.\u001b[39;49;00m\n\u001b[32m   1184\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ammad\\Documents\\Projects\\Personal\\Brain\\brain_tumor_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ammad\\Documents\\Projects\\Personal\\Brain\\brain_tumor_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1491\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1488\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._process_data(data, worker_id)\n\u001b[32m   1490\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._tasks_outstanding > \u001b[32m0\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1491\u001b[39m idx, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1492\u001b[39m \u001b[38;5;28mself\u001b[39m._tasks_outstanding -= \u001b[32m1\u001b[39m\n\u001b[32m   1493\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable:\n\u001b[32m   1494\u001b[39m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ammad\\Documents\\Projects\\Personal\\Brain\\brain_tumor_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1443\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._get_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1441\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m   1442\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory_thread.is_alive():\n\u001b[32m-> \u001b[39m\u001b[32m1443\u001b[39m         success, data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1444\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[32m   1445\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ammad\\Documents\\Projects\\Personal\\Brain\\brain_tumor_env\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:1284\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._try_get_data\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1271\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout=_utils.MP_STATUS_CHECK_INTERVAL):\n\u001b[32m   1272\u001b[39m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[32m   1273\u001b[39m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1281\u001b[39m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[32m   1282\u001b[39m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[32m   1283\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1284\u001b[39m         data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_queue\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1285\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[32m   1286\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1287\u001b[39m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[32m   1288\u001b[39m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[32m   1289\u001b[39m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python312\\Lib\\queue.py:180\u001b[39m, in \u001b[36mQueue.get\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m    178\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m remaining <= \u001b[32m0.0\u001b[39m:\n\u001b[32m    179\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m Empty\n\u001b[32m--> \u001b[39m\u001b[32m180\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnot_empty\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    181\u001b[39m item = \u001b[38;5;28mself\u001b[39m._get()\n\u001b[32m    182\u001b[39m \u001b[38;5;28mself\u001b[39m.not_full.notify()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Program Files\\Python312\\Lib\\threading.py:359\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m timeout > \u001b[32m0\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m         gotit = \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    360\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    361\u001b[39m         gotit = waiter.acquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# 1. Imports for Data Handling and Training\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# 2. Configuration and Paths\n",
    "try:\n",
    "    project_root = Path.cwd().parent \n",
    "    processed_data_dir = project_root / 'data' / 'processed'\n",
    "    if not processed_data_dir.exists():\n",
    "        processed_data_dir = Path.cwd() / 'data' / 'processed'\n",
    "except Exception:\n",
    "    processed_data_dir = Path('./data/processed')\n",
    "\n",
    "train_dir = processed_data_dir / 'train'\n",
    "val_dir = processed_data_dir / 'val'\n",
    "\n",
    "# Model & Training Hyperparameters\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 15\n",
    "LEARNING_RATE = 0.001\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "print(f\"--- Configuration ---\")\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "print(f\"Training data path: {train_dir}\")\n",
    "print(f\"Validation data path: {val_dir}\")\n",
    "print(f\"Batch Size: {BATCH_SIZE}, Epochs: {NUM_EPOCHS}, LR: {LEARNING_RATE}\")\n",
    "print(\"-\" * 21)\n",
    "\n",
    "\n",
    "# 3. Data Loading and Transformations\n",
    "# The model expects 128x128 single-channel images.\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    \n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    \n",
    "    transforms.ToTensor(), # Converts grayscale PIL image [H, W] to tensor [1, H, W] and scales to [0, 1]\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]) # Normalizes tensor to range [-1, 1]\n",
    "])\n",
    "\n",
    "# Use ImageFolder, which is perfect for your directory structure (train/class_A, train/class_B)\n",
    "try:\n",
    "    train_dataset = datasets.ImageFolder(root=train_dir, transform=data_transforms)\n",
    "    val_dataset = datasets.ImageFolder(root=val_dir, transform=data_transforms)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4, pin_memory=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "    print(\"\\n‚úÖ Data loaded successfully!\")\n",
    "    print(f\"Found classes: {train_dataset.classes} -> {train_dataset.class_to_idx}\")\n",
    "    print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\n‚ùå ERROR: Data directories not found. Please check the 'processed_data_dir' path.\")\n",
    "    print(f\"   - Searched for train data at: {train_dir}\")\n",
    "    print(f\"   - Searched for validation data at: {val_dir}\")\n",
    "    train_loader, val_loader = None, None\n",
    "\n",
    "# 4. Initialize Model, Loss Function, and Optimizer\n",
    "model = ShallowCNN(input_channels=1).to(DEVICE)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "\n",
    "# 5. Training and Validation Loop\n",
    "def run_training(model, criterion, optimizer, train_loader, val_loader, num_epochs):\n",
    "    if not train_loader or not val_loader:\n",
    "        print(\"\\nCannot start training because data loaders are not initialized.\")\n",
    "        return None, {}\n",
    "        \n",
    "    start_time = time.time()\n",
    "    history = {'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1}/{num_epochs} ---\")\n",
    "        \n",
    "        # --- Training Phase ---\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        running_corrects = 0\n",
    "        \n",
    "        for inputs, labels in tqdm(train_loader, desc=\"Training\"):\n",
    "            inputs = inputs.to(DEVICE)\n",
    "            labels = labels.float().view(-1, 1).to(DEVICE)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            preds = (outputs > 0.5).float()\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "            running_corrects += torch.sum(preds == labels)\n",
    "\n",
    "        epoch_loss = running_loss / len(train_loader.dataset)\n",
    "        epoch_acc = running_corrects.double() / len(train_loader.dataset)\n",
    "        history['train_loss'].append(epoch_loss)\n",
    "        history['train_acc'].append(epoch_acc.item())\n",
    "        print(f\"Train Loss: {epoch_loss:.4f} | Train Acc: {epoch_acc:.4f}\")\n",
    "\n",
    "        # --- Validation Phase ---\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        val_corrects = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader, desc=\"Validating\"):\n",
    "                inputs = inputs.to(DEVICE)\n",
    "                labels = labels.float().view(-1, 1).to(DEVICE)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                \n",
    "                preds = (outputs > 0.5).float()\n",
    "                val_loss += loss.item() * inputs.size(0)\n",
    "                val_corrects += torch.sum(preds == labels)\n",
    "\n",
    "        val_epoch_loss = val_loss / len(val_loader.dataset)\n",
    "        val_epoch_acc = val_corrects.double() / len(val_loader.dataset)\n",
    "        history['val_loss'].append(val_epoch_loss)\n",
    "        history['val_acc'].append(val_epoch_acc.item())\n",
    "        print(f\"Val Loss:   {val_epoch_loss:.4f} | Val Acc:   {val_epoch_acc:.4f}\")\n",
    "\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    print(f\"\\nüéâ Training Finished! Total time: {total_time // 60:.0f}m {total_time % 60:.0f}s\")\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "# 6. Start the Training Process\n",
    "trained_model, training_history = run_training(\n",
    "    model, \n",
    "    criterion, \n",
    "    optimizer, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=NUM_EPOCHS\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1fe7d95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_breasthisto_subtypes(processed_data_dir, n_samples=5):\n",
    "    \"\"\"Visualizes and compares IDC positive and negative samples.\"\"\"\n",
    "    # Ensure processed_data_dir is a Path object\n",
    "    processed_data_dir = Path(processed_data_dir)\n",
    "    breasthisto_dir = processed_data_dir / 'train' / 'BreastHisto'\n",
    "    \n",
    "    if not breasthisto_dir.exists():\n",
    "        print(f\"‚ùå Directory not found: {breasthisto_dir}\")\n",
    "        print(\"Please ensure your data is in the correct directory structure.\")\n",
    "        return\n",
    "        \n",
    "    all_files = list(breasthisto_dir.glob('*.png'))\n",
    "    positive_files = [f for f in all_files if 'IDC_positive' in f.name]\n",
    "    negative_files = [f for f in all_files if 'IDC_negative' in f.name]\n",
    "    \n",
    "    if len(positive_files) < n_samples or len(negative_files) < n_samples:\n",
    "        print(f\"‚ö†Ô∏è Not enough positive or negative samples to display.\")\n",
    "        print(f\"Found {len(positive_files)} positive and {len(negative_files)} negative samples.\")\n",
    "        return\n",
    "\n",
    "    sample_pos = random.sample(positive_files, n_samples)\n",
    "    sample_neg = random.sample(negative_files, n_samples)\n",
    "\n",
    "    fig, axes = plt.subplots(2, n_samples, figsize=(15, 6))\n",
    "    fig.suptitle('Breast Histopathology Subtypes (Train Set)', fontsize=16)\n",
    "\n",
    "    for i in range(n_samples):\n",
    "        # Positive samples\n",
    "        img_pos = Image.open(sample_pos[i])\n",
    "        axes[0, i].imshow(img_pos)\n",
    "        axes[0, i].set_title(f'IDC Positive\\n{img_pos.size[0]}x{img_pos.size[1]}')\n",
    "        axes[0, i].axis('off')\n",
    "\n",
    "        # Negative samples\n",
    "        img_neg = Image.open(sample_neg[i])\n",
    "        axes[1, i].imshow(img_neg)\n",
    "        axes[1, i].set_title(f'IDC Negative\\n{img_neg.size[0]}x{img_neg.size[1]}')\n",
    "        axes[1, i].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "    plt.show()\n",
    "\n",
    "# --- Example Usage for Visualization ---\n",
    "# NOTE: You must define the path to your processed data directory.\n",
    "# This is a placeholder and will likely fail if not changed.\n",
    "try:\n",
    "    # Create a dummy path for demonstration purposes\n",
    "    processed_data_dir = Path('./processed_data') \n",
    "    \n",
    "    # You would call the function like this, assuming your data exists\n",
    "    # and split_statistics is a boolean you've defined elsewhere.\n",
    "    split_statistics = True # Assuming this condition is met\n",
    "    if split_statistics:\n",
    "        visualize_breasthisto_subtypes(processed_data_dir, n_samples=5)\n",
    "except Exception as e:\n",
    "    print(f\"\\nCould not run visualization. Error: {e}\")\n",
    "    print(\"Please update 'processed_data_dir' to your actual data path.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "brain_tumor_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
